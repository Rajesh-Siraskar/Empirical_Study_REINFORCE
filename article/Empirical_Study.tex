%% (2) Build options: PdfLaTex + BibTex
\documentclass[a4paper, 12pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[round]{natbib}
\bibliographystyle{plainnat}

\usepackage[pass]{geometry}
\usepackage{graphicx}
\graphicspath{{./images/}} % Images folder

%\usepackage{csquotes} 	% Quotes
\usepackage[table]{xcolor} %
\usepackage{booktabs}	% Tables
\usepackage{arydshln} 	% dashed lines in tables
\usepackage{pdflscape}	% Landscape page

\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsfonts}
\usepackage{amsmath,amssymb}
\usepackage{textcomp}	% for \texttrademark
\usepackage{setspace}
\usepackage[spaces, hyphens]{url}
\usepackage[colorlinks, allcolors=blue]{hyperref} 

% Macros
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcommand{\rowspace}[1]{\renewcommand{\arraystretch}{#1}}
\renewenvironment{abstract}
{\small
	\begin{center}
		\bfseries \abstractname\vspace{-.5em}\vspace{0pt}
	\end{center}
	\list{}{
		\setlength{\leftmargin}{.25cm}%
		\setlength{\rightmargin}{\leftmargin}%
	}%
	\item\relax}
{\endlist}

%% Quotes with epigraph style
\usepackage{epigraph}
% \epigraphsize{\small}% Default
\setlength\epigraphwidth{12cm}
\setlength\epigraphrule{0pt}
\usepackage{etoolbox}
\makeatletter
\patchcmd{\epigraph}{\@epitext{#1}}{\itshape\@epitext{#1}}{}{}
\makeatother
%% End Quotes macro

\title{An empirical study of the na\"ive REINFORCE algorithm for predictive maintenance of industrial milling machines}
\author{Rajesh Siraskar}

\onehalfspacing

\begin{document}
\maketitle

\begin{abstract}
In this empirical study, we document the performance of a simple, early reinforcement learning (RL) algorithm, REINFORCE, implemented for a predictive maintenance problem -- an optimal tool replacement policy for a milling machine. We compare a na\"ive implementation of REINFORCE against the policies of industry-grade implementations (Stable-Baselines3) of three advanced algorithms, namely, Deep Q-Network (DQN), Advantage Actor-Critic (A2C) and Proximal Policy Optimization (PPO). This work is aimed at industrial practitioners not accustomed to the complex hyperparameter tuning often required to get RL to work. Our broad goal was to understand the performance of \textit{untuned} algorithms under various scenarios: (1) simulation tool-wear data (2) real tool-wear data (benchmark IEEE NUAA Ideahouse dataset) (3) added noise levels and a random chance of break-down. 

Model performance was measured by how accurately the predictive maintenance agent suggested tool replacement when compared to a deterministic preventive maintenance rule based on the tool-wear threshold. Across variants of the environment, REINFORCE models demonstrated a tool replacement precision of 0.687 against 0.449 for A2C, 0.418 for DQN, and 0.472 for PPO. The F1 scores were 0.609, 0.442, 0.374 and 0.345 respectively. Variability in precision and F1 was lower for REINFORCE by 0.08 and 0.016, when compared to the average of the three advanced algorithms. Comparing the \textit{best model} over 10 rounds of training produced surprisingly larger gaps in performance. REINFORCE precision/F1 stood at 0.884/0.873. The best A2C, DQN and PPO models produced 0.520/0.639, 0.651/0.740 and 0.558/0.580 respectively. Our findings indicate that the computationally lightweight REINFORCE performs significantly well for this particular problem. Consequently, for this particular problem, selecting the na\"ive REINFORCE could be a more suitable and effective policy generating alternative to more advanced complex algorithms. 

For reproducibility, model training and testing code, data and the trained REINFORCE models have been uploaded to \href{https://github.com/Rajesh-Siraskar/Empirical-Study\_REINFORCE-for-predictive-maintenance}{https://github.com/Link} 
\end{abstract}

\noindent \textbf{Keywords}: Predictive maintenance, milling machines, Reinforcement Learning, REINFORCE

\section*{Abbreviations}

\begin{table*}[!htbp]\centering
	\sffamily
	\rowspace{1.3}
	\begin{tabular}{L{1cm} L{4.5cm} L{1cm} L{4.5cm}}
		\arrayrulecolor{black!40}\toprule	
		
		DQN & Deep Q-Network & A2C & Advantage Actor-Critic\\
		PPO & Proximal Policy Optimization & RF & REINFORCE\\
		SS & Single-variable state & MS & Multi-variate state\\
		TP &True positive &TN &True negative\\
		FP &False positive &FN &False negative\\
		RL & Reinforcement Learning & SB3 & Stable-Baselines3\\
		PHM & The Prognostics and Health Management Society & \textcolor{red}{*AA*} & \textcolor{red}{Advanced Algorithms} \\

		\bottomrule
	\end{tabular}
	\label{tbl:abbrev}
\end{table*}

\newpage
\section{Introduction}
\epigraph{"Plurality should not be posited without necessity" -- Of two competing theories, the simpler explanation of an entity is to be preferred}{--- \textup{William of Ockham (1285â€“1347)}, The Occams razor principle}

\noindent Milling machines are highly versatile, ubiquitous tools serving a variety of industries. A milling machine removes metal from the work piece by rotating and driving a cutting device into it. Abrasive forces cause tool wear, and optimal tool replacement reduces direct costs and optimizes the machines' downtime. With the 2023 milling machine market valued at USD 68.3 billion \citep{milling-market}, this is an important goal for the industry. The cutting tool experiences multiple types of wear as it cuts through metal. Tool wear depends in several factors such as the cutting speed, force applied to the tool, lubrication and materials of the work piece and cutting tool. 

Reinforcement learning (RL) is an artificial intelligence technique inspired by nature. Fig. \ref{fig:RL-loop} \citep{barto2018} shows the RL learning feedback loop. An actor or ``agent" interacts with an environment and learns via ``trial-and-error". It acts based on stimuli or feedback received from the environment after performing a certain action. Actions that help in achieving the learning goal receive a reward while actions that do not, are punished. Repeating this loop over thousands of episodes, good actions are ``reinforced", thereby building a ``policy" that is optimized for that goal. In the case of predictive maintenance for milling machines, the agent is the ``planner" with a goal of learning an optimal tool replacement policy. The environment consists of sensors attached to the machine and related information such as job specifications, environment conditions etc.

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.7\textwidth]{RL-loop.pdf}
	\caption{Reinforcement Learning}
	\label{fig:RL-loop}
\end{figure}

Introduced in 1992, the REINFORCE algorithm \citep{REINFORCE-williams1992} is considered as a basic reinforcement learning algorithm. It is a policy-based, on-policy algorithm, capable of handling both discrete and continuous observation and action domains.

In practice the REINFORCE algorithm is considered as a ``weak" learner and superseded by several algorithms developed since. Most notably the Q-Learning and its deep-neural network version, the DQN \citep{DQN-mnih2013}, followed by Actor-Critic \citep{A2C-mnih2016} and one of the most robust modern-day algorithms, the PPO \citep{PPO-schulman2017}.

\subsection{Stable-Baselines3}
Stable-Baselines3 \cite{SB3-paper}, is Open Source and very popular among the RL community\footnote{As of 27-Jun-2023, it had 6k+ stars and 424 closed pull requests}. Stable-Baselines3 was initially based on the Open AI baselines \citep{OpenAI-baselines} and is completely rewritten using PyTorch. 


 for  , implementation s  the advanced algorithms -- DQN, A2C and PPO and compare its performance to a na\"ive custom implementation of the REINFORCE algorithm. We use Stable-Baselines3 (SB3), the highly popular and reliable implementations of DQN, A2C and PPO. As of 27-Jun-2023, the REINFORCE was not implemented by Stable-Baselines and we therefore custom implemented a basic version. 

\subsection{DQN}
Deep Q Network (DQN) builds on Fitted Q-Iteration (FQI) \citep{riedmiller2005neural} and make use of different tricks to stabilize the learning with neural networks: it uses a replay buffer, a target network and gradient clipping.

%SB3 settings > classstable_baselines3.dqn.DQN(policy, env, learning_rate=0.0001, buffer_size=1000000, learning_starts=50000, batch_size=32, tau=1.0, gamma=0.99, train_freq=4, gradient_steps=1, replay_buffer_class=None, replay_buffer_kwargs=None, optimize_memory_usage=False, target_update_interval=10000, exploration_fraction=0.1, exploration_initial_eps=1.0, exploration_final_eps=0.05, max_grad_norm=10, stats_window_size=100, tensorboard_log=None, policy_kwargs=None, verbose=0, seed=None, device='auto', _init_setup_model=True) %https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html

\subsection{REINFORCE}
sB-3 dOES NOT IMPLEMENT THE REINFORCE	 --- https://stable-baselines3.readthedocs.io/en/master/guide/algos.html

Model Free --> Policy Gradient/Actor-Critic --> REINFORCE)
REINFORCE (Monte-Carlo policy gradient) 


IEEE NUAA Ideahouse dataset has been used in this paper for the RUL estimation of the milling cutter. \cite{NUAA-data-set}



Tool wear modeling is the first step to assist in predicting 


Literature search conducted on the Scopus\texttrademark{} and Web Of Science\texttrademark{} did not return any articles for the application of reinforcement learning for predictive maintenance of milling machines. Search strings we tried -- \texttt{``reinforcement learning AND tool wear AND maintenance", RL + milling + policy, RL + milling + maintenance, RL + tool wear + policy, RL + tool wear + maintenance"} 

Running the search \texttt{``reinforcement learning AND milling AND tool wear"} using the Scopus\texttrademark{} and Web Of Science\texttrademark{} services 

\cite{dai2021reinforcement} is the only article we found that tackles the 

Machine learning methods have been applied for example --
\cite{oshida2023development} proposes real-time tool wear detection during the milling process. They use a stacked LSTM encoder-decoder model for anomaly detection.


No results for "reinforcement learning" AND "milling machine" AND "tool wear" - on scopus or wos as of 23-jun-2023
"reinforcement learning" AND "milling machine" - 1 not relevant "	
\textit{Conference Paper}  â€¢  Open access "Online Learning of Stability Lobe Diagrams in Milling" 	
Friedrich, J. Torzewski, J.  Verl, A.

"reinforcement learning" AND "tool wear" - 10 results


\section{Literature Review}

\section{Method}

\textit{The methodology explains in detail what the researcher did to undertake the research. Various aspects of the research have to be outlined: The overall structure and operation of the experiment or observational experience. The groups studied in the research including the size of each group and any features of the subjects which may be relevant to the topic being researched. The variables that were changed between groups and the variables measured as a result of the changes. The conditions under which the research was undertaken and any factors or variations in conditions which may have an impact on the results. The methods of data analysis used in order to analyse and collate the results. \textcolor{red}{***}Any limitations of the data collected.}
We normalize the tool wear and other state features, $x \in [0,\;1] \subset \mathbb{R} $. This allows for adding white noise of similar magnitudes across experiments of different data-sets
\subsection{RL environment description}
\subsection{Data description}
\subsection{Procedure}
 -- training \\
 -- selecting the model\\
 -- conducting the experiments\\
 
\subsection{Evaluation method}
\subsection{Tools}
2 different laptops

\begin{itemize}
	\item why classifction metrics
	\item why F1beta	
\end{itemize}

\subsection{Method - training and testing}
\subsection{Precision or Recall?}
- Precision => low FP => False replacement-action reduced. \textcolor{red}{***} Unnecessary tool replacement reduced. Tool life maximized, down time minimized, production disruption minimized \\
- Recall => low FN => False declaration of normal operation reduced. Reduce missed replacements. Tool replacements increased. \textcolor{red}{***} Product quality not compromised. 
\subsection{Hyper-parameters for Precision or Recall control}

\begin{itemize}
	\item R1 = +1 
	\item R2 = -1
	\item R3 = -100 => higher neg. Improve recall. Lower neg. Improve precision
	\item \textbf{LOOK AHEAD PARAM}: 
\end{itemize}

\begin{itemize}
	\item Training: SB3 - 10 k eps. 3 times. Average their outputs
	\item Testing: 
	\begin{itemize}
		\item Avg. over 5 rounds.
		\item Each round - avg over 40 test cases x 10 test rounds
		\item Total: 40 x 10 x 5 = 2000 cases
		\item Avgs over: 10 rounds (of 40 cases each) X 5 rounds of \textbf{re-trained} SB3 agents = 50 rounds 
	\end{itemize}
\end{itemize}

\subsection{Inference}
\begin{itemize}
	\item Training: SB3 is also unstable - show examples of results such as A2C/DQN 0.00 
	\item Training: SB3 is also unstable - SHOW SB3 tensorboard plots
	\item Training: SB3 is also unstable - EXCEL plots of results over the 10 rounds 	
\end{itemize}


\section{Network architecture and basic hyper-parameters}

\begin{table*}\centering
	\sffamily
	\rowspace{1.5}
	\begin{tabular}{L{2cm} R{2.5cm} R{2.5cm} R{2.5cm} R{3cm}}
		\arrayrulecolor{black!40}\toprule
		&\textbf{A2C}&\textbf{DQN}&\textbf{PPO}&\textbf{REINFORCE}\\ \midrule
		
		Network\par architecture&input dim x\par [64|Tanh x 64|Tanh]\par x output dim&input dim x\par [64|Tanh x 64|Tanh]\par x output dim&input dim x\par [64|Tanh x 64|Tanh]\par x output dim&input dim x\par [64|ReLU]\par x output dim\\
		Layers&2&2&2&1\\
		Units&64  x 64&64  x 64&64  x 64&64\\
		Activation&Tanh, Tanh&Tanh, Tanh&Tanh, Tanh&ReLU\\
		Optimizer&RMSprop&Adam&Adam&Adam\\ \midrule
		Learning rate&0.0007&0.0001&0.0003&0.01\\
		Gamma&0.99&0.99&0.99&0.99\\			
		\bottomrule
	\end{tabular}
	\caption{Comparing the network architecture and basic hyper-parameters across algorithms}
	\label{tbl:hyperparameters}
\end{table*}




soure of ppo implementation details \url{https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/}

source of SB3 network mp : \url{https://github.com/openai/baselines/blob/ea25b9e8b234e6ee1bca43083f8f3cf974143998/baselines/common/models.py#L75-L103}

\subsection{PPO hyperparms}

implementation guide source >> https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/

By default, PPO uses a simple MLP network consisting of two layers of 64 neurons and Hyperbolic Tangent as the activation function. Then PPO builds a policy head and value head that share the outputs of the MLP network. Below is a pseudocode:
%\textt{
%network = Sequential(
%layer_init(Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
%Tanh(),
%layer_init(Linear(64, 64)),
%Tanh(),
%)
%value_head = layer_init(Linear(64, 1), std=1.0)
%policy_head = layer_init(Linear(64, envs.single_action_space.n), std=0.01)
%hidden = network(observation)
%value = value_head(hidden)
%action = Categorical(policy_head(hidden)).sample()}

\subsection{dqn hyperparms}


default hyperparms : \url{https://stable-baselines3.readthedocs.io/en/master/_modules/stable_baselines3/common/policies.html}

overridden in indiv policies for example 
SB3 DQN hypoerparms for example were taken from ;
Paper: https://arxiv.org/abs/1312.5602, https://www.nature.com/articles/nature14236
Default hyperparameters are taken from the Nature paper,
except for the optimizer and learning rate that were taken from Stable Baselines defaults


\section{Empirical results}

\begin{figure}[ht]
	\begin{subfigure}{\textwidth}
	\centering
	\includegraphics[width=\linewidth]{Overall_Pr.pdf}  
	\caption{Precision}
	\label{fig:tr-ovr-pr}
	\end{subfigure} \par\smallskip

	\begin{subfigure}{\textwidth}
	\centering
	\includegraphics[width=\linewidth]{Overall_Rc.pdf}  
	\caption{Recall}
	\label{fig:tr-ovr-rc}
	\end{subfigure} \par\smallskip
	
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Overall_F1.pdf}  
		\caption{F1-score}
		\label{fig:tr-ovr-f1}
	\end{subfigure} \par\smallskip

	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Overall_F05.pdf}  
		\caption{F1-beta (0.5)}
		\label{fig:tr-ovr-f05}
	\end{subfigure}
	\caption{Overall performance -- Avg. performance over 10 rounds of model training}
	\label{fig:tr-overall}
\end{figure}

\begin{figure}[ht]
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Simulated_Pr.pdf}  
		\caption{Precision}
		\label{fig:tr-sim-pr}
	\end{subfigure} \par\smallskip
	
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Simulated_Rc.pdf}  
		\caption{Recall}
		\label{fig:tr-sim-rc}
	\end{subfigure} \par\smallskip
	
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Simulated_F1.pdf}  
		\caption{F1-score}
		\label{fig:tr-sim-f1}
	\end{subfigure} \par\smallskip
	
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Simulated_F05.pdf}  
		\caption{F1-beta (0.5)}
		\label{fig:tr-sim-f05}
	\end{subfigure}
	\caption{Simulated environment -- Avg. performance over 10 rounds of model training}
	\label{fig:tr-sim-env}
\end{figure}

\begin{figure}[ht]
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Singevariable_Pr.pdf}  
		\caption{Precision}
		\label{fig:tr-ss-pr}
	\end{subfigure} \par\smallskip
	
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Singevariable_Rc.pdf}  
		\caption{Recall}
		\label{fig:tr-ss-rc}
	\end{subfigure} \par\smallskip
	
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Singevariable_F1.pdf}  
		\caption{F1-score}
		\label{fig:tr-ss-f1}
	\end{subfigure} \par\smallskip
	
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Singevariable_F05.pdf}  
		\caption{F1-beta (0.5)}
		\label{fig:tr-ss-f05}
	\end{subfigure}
	\caption{Singe-variable state environment -- Avg. performance over 10 rounds of model training}
	\label{fig:tr-ss-env}
\end{figure}

\begin{figure}[ht]
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Multivariate_Pr.pdf}  
		\caption{Precision}
		\label{fig:tr-ms-pr}
	\end{subfigure} \par\smallskip
	
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Multivariate_Rc.pdf}  
		\caption{Recall}
		\label{fig:tr-ms-rc}
	\end{subfigure} \par\smallskip
	
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Multivariate_F1.pdf}  
		\caption{F1-score}
		\label{fig:tr-ms-f1}
	\end{subfigure} \par\smallskip
	
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Multivariate_F05.pdf}  
		\caption{F1-beta (0.5)}
		\label{fig:tr-ms-f05}
	\end{subfigure}
	\caption{Multi-variate state environment -- Avg. performance over 10 rounds of model training}
	\label{fig:tr-ms-env}
\end{figure}

\begin{figure}[ht]
	\centering
	\includegraphics[width=\linewidth]{Model_training_time.pdf}  
	\caption{Training time. Across 10 rounds and all environment variants.}
	\label{fig:tr-time}
\end{figure}

\subsection{Detailed metrics}
xx
\newgeometry{margin=1cm} % Change margins for landscape table
\begin{landscape}\centering
	\begin{table*}
		\sffamily
		\rowspace{1.3}
		\begin{tabular}{@{}l rrrr c rrrr c rrrr c rrrr@{}} \arrayrulecolor{black!40}\toprule
			& \multicolumn{4}{c}{\textbf{REINFORCE}} & & \multicolumn{4}{c}{A2C} &
			& \multicolumn{4}{c}{DQN} & & \multicolumn{4}{c}{PPO} \\
			\cmidrule{2-5} \cmidrule{7-10} \cmidrule{12-15} \cmidrule{17-20}
			Environment &Prec. &Recall &F1 &F0.5 & &Prec. &Recall &F1 &F0.5 & &Prec. &Recall &F1 &F0.5 & &Prec. &Recall &F1 &F0.5\\ \midrule
			Simulated  - No noise &0.842 &0.878 &0.838 & 0.834 & & 0.424 &0.451 &0.423 &0.421 & &0.426 &0.674 &0.471 &0.410 & &0.504 &0.200 &0.271&0.360\\
			Simulated  - Low noise &0.777 &0.929 &0.834 & 0.796 & & 0.465 &0.423 &0.409 &0.427 & &0.421 &0.338 &0.270 &0.283 & &0.482 &0.236 &0.296&0.369\\
			Simulated  - High noise &0.798 &0.940 &0.851 & 0.816 & & 0.358 &0.281 &0.256 &0.272 & &0.447 &0.519 &0.380 &0.360 & &0.514 &0.207 &0.286&0.382\\ \midrule
			
			PHM C01 SS - No noise &0.478 &0.363 &0.400 & 0.439 & & 0.501 &0.500 &0.493 &0.496 & &0.472 &0.807 &0.568 &0.490 & &0.440 &0.417 &0.387&0.395\\
			PHM C01 SS - Low noise &0.507 &0.311 &0.332 & 0.383 & & 0.503 &0.598 &0.535 &0.513 & &0.393 &0.502 &0.351 &0.317 & &0.522 &0.338 &0.388&0.448\\
			PHM C01 SS - High noise &0.693 &0.562 &0.579 & 0.623 & & 0.266 &0.282 &0.267 &0.262 & &0.458 &0.525 &0.400 &0.384 & &0.456 &0.369 &0.372&0.400\\ \hdashline
			
			PHM C04 SS - No noise &0.751 &0.878 &0.784 & 0.757 & & 0.487 &0.442 &0.449 &0.463 & &0.439 &0.684 &0.472 &0.411 & &0.500 &0.510 &0.469&0.473\\
			PHM C04 SS - Low noise &0.662 &0.756 &0.672 & 0.657 & & 0.409 &0.455 &0.428 &0.416 & &0.411 &0.500 &0.370 &0.341 & &0.488 &0.280 &0.324&0.386\\
			PHM C04 SS - High noise &0.611 &0.713 &0.620 & 0.598 & & 0.518 &0.607 &0.552 &0.530 & &0.358 &0.451 &0.325 &0.294 & &0.428 &0.262 &0.286&0.333\\ \hdashline
			
			PHM C06 SS - No noise &0.830 &0.726 &0.754 & 0.792 & & 0.517 &0.509 &0.507 &0.511 & &0.360 &0.309 &0.256 &0.258 & &0.409 &0.248 &0.275&0.321\\
			PHM C06 SS - Low noise &0.205 &0.279 &0.228 & 0.212 & & 0.510 &0.577 &0.530 &0.516 & &0.434 &0.266 &0.266 &0.296 & &0.417 &0.181 &0.232&0.294\\
			PHM C06 SS - High noise &0.709 &0.843 &0.759 & 0.726 & & 0.316 &0.324 &0.311 &0.308 & &0.449 &0.518 &0.400 &0.375 & &0.388 &0.222 &0.265&0.317\\ \midrule
			
			PHM C01 MS - No noise &0.835 &0.652 &0.656 & 0.716 & & 0.461 &0.444 &0.397 &0.404 & &0.384 &0.558 &0.393 &0.348 & &0.513 &0.383 &0.416&0.460\\
			PHM C04 MS - No noise &0.739 &0.255 &0.359 & 0.494 & & 0.498 &0.589 &0.490 &0.470 & &0.323 &0.209 &0.160 &0.168 & &0.499 &0.393 &0.421&0.457\\
			PHM C06 MS - No noise &0.864 &0.356 &0.469 & 0.616 & & 0.501 &0.713 &0.578 &0.527 & &0.489 &0.705 &0.529 &0.479 & &0.523 &0.488 &0.485&0.498\\
			
			\bottomrule
		\end{tabular}
		\caption{Model performance comparison all variants of the environments, over 10 rounds of training.}
		\label{tbl:DetailedMetrics}
	\end{table*}
\end{landscape}
\restoregeometry % Restore margins after landscape table


\subsection{Super models metrics}

\newgeometry{margin=1cm} % Change margins for landscape table
\begin{landscape}\centering
	\begin{table*}
		\sffamily
		\rowspace{1.3}
		\begin{tabular}{@{}l rrrr c rrrr c rrrr c rrrr@{}} \arrayrulecolor{black!40}\toprule
			& \multicolumn{4}{c}{\textbf{REINFORCE}} & & \multicolumn{4}{c}{A2C} &
			& \multicolumn{4}{c}{DQN} & & \multicolumn{4}{c}{PPO} \\
			\cmidrule{2-5} \cmidrule{7-10} \cmidrule{12-15} \cmidrule{17-20}
			Environment &Prec. &Recall &F1 &F0.5 & &Prec. &Recall &F1 &F0.5 & &Prec. &Recall &F1 &F0.5 & &Prec. &Recall &F1 &F0.5\\ \midrule
			Simulated  - No noise &0.897 &0.960 &0.926 & 0.908 & & 0.500 &1.000 &0.667 &0.556 & &0.505 &0.980 &0.667 &0.560 & &0.669 &0.430 &0.518&0.597\\
			Simulated  - Low noise &0.960 &0.945 &0.952 & 0.957 & & 0.516 &1.000 &0.680 &0.571 & &0.500 &0.980 &0.662 &0.554 & &0.633 &0.460 &0.530&0.586\\
			Simulated  - High noise &0.922 &0.990 &0.955 & 0.935 & & 0.503 &1.000 &0.669 &0.558 & &0.504 &0.990 &0.668 &0.559 & &0.569 &0.355 &0.434&0.505\\\midrule
			
			PHM C01 SS - No noise &0.889 &0.995 &0.939 & 0.908 & & 0.586 &0.625 &0.603 &0.592 & &0.647 &0.970 &0.776 &0.693 & &0.543 &1.000 &0.703&0.597\\
			PHM C01 SS - Low noise &0.988 &0.765 &0.861 & 0.932 & & 0.499 &0.995 &0.664 &0.554 & &0.504 &0.990 &0.668 &0.559 & &0.623 &0.740 &0.675&0.643\\
			PHM C01 SS - High noise &0.850 &0.970 &0.905 & 0.871 & & 0.521 &0.680 &0.588 &0.546 & &0.505 &0.985 &0.668 &0.560 & &0.520 &0.725 &0.604&0.551\\\hdashline
			
			PHM C04 SS - No noise &0.811 &1.000 &0.895 & 0.842 & & 0.536 &0.645 &0.583 &0.554 & &0.501 &0.965 &0.660 &0.554 & &0.579 &0.895 &0.702&0.622\\
			PHM C04 SS - Low noise &0.798 &0.980 &0.879 & 0.829 & & 0.556 &0.665 &0.603 &0.573 & &0.734 &0.990 &0.843 &0.774 & &0.546 &0.660 &0.596&0.565\\
			PHM C04 SS - High noise &0.708 &0.840 &0.767 & 0.730 & & 0.521 &0.835 &0.641 &0.563 & &0.511 &0.985 &0.672 &0.565 & &0.517 &0.820 &0.633&0.558\\\hdashline
			
			PHM C06 SS - No noise &1.000 &0.895 &0.944 & 0.977 & & 0.520 &0.680 &0.587 &0.545 & &0.935 &0.975 &0.954 &0.942 & &0.587 &0.650 &0.615&0.597\\
			PHM C06 SS - Low noise &0.943 &0.795 &0.861 & 0.908 & & 0.501 &1.000 &0.668 &0.557 & &0.961 &0.725 &0.826 &0.901 & &0.552 &0.370 &0.438&0.497\\
			PHM C06 SS - High noise &0.821 &0.845 &0.831 & 0.825 & & 0.540 &0.755 &0.628 &0.572 & &0.980 &0.960 &0.969 &0.976 & &0.521 &0.615 &0.564&0.537\\\midrule
			
			PHM C01 MS - No noise &0.827 &0.995 &0.903 & 0.856 & & 0.500 &1.000 &0.667 &0.556 & &0.505 &0.985 &0.668 &0.560 & &0.512 &0.595 &0.549&0.526\\
			PHM C04 MS - No noise &0.910 &0.425 &0.577 & 0.738 & & 0.500 &1.000 &0.667 &0.556 & &0.501 &0.975 &0.662 &0.555 & &0.501 &0.635 &0.558&0.522\\
			PHM C06 MS - No noise &0.934 &0.865 &0.896 & 0.918 & & 0.500 &1.000 &0.667 &0.556 & &0.969 &0.600 &0.741 &0.863 & &0.497 &0.690 &0.577&0.526\\			
			\bottomrule
		\end{tabular}
		\caption{Super Models: Best models selected over 10 rounds of training.}
		\label{tbl:SuperModels}
	\end{table*}
\end{landscape}
\restoregeometry % Restore margins after landscape table

\subsection{Overall summary performance}

\begin{table*}[hbt!]\centering
	\sffamily
	\rowspace{1.3}
	\begin{tabular}{@{}l rr c rr c rr c rr@{}}
		\arrayrulecolor{black!40}\toprule
		& \multicolumn{2}{c}{Precision} & \phantom{i} & \multicolumn{2}{c}{Recall} & \phantom{i} & \multicolumn{2}{c}{F1-score} & \phantom{i} & \multicolumn{2}{c}{F1-beta score} \\
		\cmidrule{2-3} \cmidrule{5-6} \cmidrule{8-9} \cmidrule{11-12} 
		
		&Mean &SD & &Mean &SD & &Mean &SD& &Mean & SD\\ \midrule
		
		A2C & 0.449 & 0.088 & &0.480 & 0.084 & & 0.442 & 0.070 & &0.436 &0.071 \\
		DQN & 0.418 & 0.185 & &0.504 & 0.032 & & 0.374 & 0.035 & &0.348 &0.058 \\
		PPO & 0.472 & 0.144 & &0.316 & 0.087 & & 0.345 & 0.091 & &0.393 &0.105 \\
		REINFORCE & 0.687 & 0.059 & &0.629 & 0.051 & & 0.609 & 0.050 & &0.631 &0.052 \\
			
		\bottomrule
	\end{tabular}
	\caption{Model performance summary - averaged over all environment.}
	\label{tbl:OverallSummary}
\end{table*}

\subsection{Simulated environment}
\begin{table*}[hbt!]\centering\sffamily
	\rowspace{1.3}
	\begin{tabular}{@{}l rr c rr c rr c rr@{}}
		\arrayrulecolor{black!40}\toprule
		& \multicolumn{2}{c}{Precision} & \phantom{i} & \multicolumn{2}{c}{Recall} & \phantom{i} & \multicolumn{2}{c}{F1-score} & \phantom{i} & \multicolumn{2}{c}{F1-beta score} \\
		\cmidrule{2-3} \cmidrule{5-6} \cmidrule{8-9} \cmidrule{11-12} 
		
		&Mean &SD & &Mean &SD & &Mean &SD& &Mean & SD\\ \midrule
		A2C & 0.416 & 0.120 & &0.385 & 0.073 & & 0.363 & 0.072 & &0.373 &0.082 \\
		DQN & 0.432 & 0.184 & &0.510 & 0.031 & & 0.374 & 0.034 & &0.351 &0.056 \\
		PPO & 0.500 & 0.178 & &0.215 & 0.081 & & 0.285 & 0.099 & &0.370 &0.122 \\
		REINFORCE & 0.806 & 0.040 & &0.915 & 0.038 & & 0.841 & 0.035 & &0.816 &0.037 \\
		
		
		\bottomrule
	\end{tabular}
	\caption{Model performance summary - averaged over simulated environments.}
	\label{tbl:SimulatedEnv}
\end{table*}

\newpage
\subsection{Real data -- simple uni-variate environment}
\begin{table*}[h]\centering
	\sffamily
	\rowspace{1.3}
	\begin{tabular}{@{}l rr c rr c rr c rr@{}}
		\arrayrulecolor{black!40}\toprule
		& \multicolumn{2}{c}{Precision} & \phantom{i} & \multicolumn{2}{c}{Recall} & \phantom{i} & \multicolumn{2}{c}{F1-score} & \phantom{i} & \multicolumn{2}{c}{F1-beta score} \\
		\cmidrule{2-3} \cmidrule{5-6} \cmidrule{8-9} \cmidrule{11-12} 
		
		&Mean &SD & &Mean &SD & &Mean &SD& &Mean & SD\\ \midrule
		A2C & 0.447 & 0.077 & &0.477 & 0.091 & & 0.452 & 0.072 & &0.446 &0.070 \\
		DQN & 0.419 & 0.179 & &0.507 & 0.032 & & 0.379 & 0.036 & &0.352 &0.057 \\
		PPO & 0.450 & 0.146 & &0.314 & 0.082 & & 0.333 & 0.087 & &0.374 &0.102 \\
		REINFORCE & 0.605 & 0.046 & &0.603 & 0.046 & & 0.570 & 0.041 & &0.576 &0.040 \\
		
		\bottomrule
	\end{tabular}
	\caption{Model performance summary - averaged over PHM-2010 environments with simple single-variable environment.}
	\label{tbl:PHMSS}
\end{table*}

\subsection{Real data -- complex multi-variate state}
\begin{table*}[hbt!]\centering
	\sffamily
	\rowspace{1.3}
	\begin{tabular}{@{}l rr c rr c rr c rr@{}}
		\arrayrulecolor{black!40}\toprule
		& \multicolumn{2}{c}{Precision} & \phantom{i} & \multicolumn{2}{c}{Recall} & \phantom{i} & \multicolumn{2}{c}{F1-score} & \phantom{i} & \multicolumn{2}{c}{F1-beta score} \\
		\cmidrule{2-3} \cmidrule{5-6} \cmidrule{8-9} \cmidrule{11-12} 
		
		&Mean &SD & &Mean &SD & &Mean &SD& &Mean & SD\\ \midrule
		A2C & 0.487 & 0.086 & &0.582 & 0.075 & & 0.488 & 0.063 & &0.467 &0.065 \\
		DQN & 0.399 & 0.204 & &0.491 & 0.032 & & 0.361 & 0.035 & &0.332 &0.060 \\
		PPO & 0.512 & 0.107 & &0.422 & 0.107 & & 0.441 & 0.096 & &0.472 &0.096 \\
		REINFORCE & 0.813 & 0.119 & &0.421 & 0.079 & & 0.495 & 0.090 & &0.609 &0.101 \\
		
		
		\bottomrule
	\end{tabular}
	\caption{Model performance summary - averaged over PHM-2010 environments with complex multi-variate environment.}
	\label{tbl:PHMMS}
\end{table*}

\subsection{Hypothesis testing}

\begin{equation}
	\left.\begin{aligned}
		H_0 & : \mu_{RF} - \mu_{AA} = 0,\;\; \\
		H_a & : \mu_{RF} - \mu_{AA} > 0, \;\;
	\end{aligned}
	\right\}
	\qquad \forall \;\; \text{$AA \in[A2C, DQN, PPO]$}
\end{equation}

\begin{table*}[hbt!]\centering
	\sffamily
	\rowspace{1.3}
	\begin{tabular}{@{}l c rrr c l rrr @{}}
		\arrayrulecolor{black!40}\toprule
		
		&& \multicolumn{3}{c}{\textbf{p Value}} & \phantom{i} & & \multicolumn{3}{c}{\textbf{t Statistic}} \\
		\cmidrule{3-5} \cmidrule{8-10} 
		
		Metric && \small {RF $\underset{H_0}{\overset{H_a}{\geqq}}$ A2C} &\small {RF $\underset{H_0}{\overset{H_a}{\geqq}}$DQN} &\small {RF $\underset{H_0}{\overset{H_a}{\geqq}}$PPO} & & & \small {RF $\underset{H_0}{\overset{H_a}{\geqq}}$ A2C} &\small {RF $\underset{H_0}{\overset{H_a}{\geqq}}$DQN} &\small {RF $\underset{H_0}{\overset{H_a}{\geqq}}$PPO} \\ \midrule 
		\multicolumn{10}{@{}l}{\textbf{Overall} (1500 samples)} \\[6pt]
		Precision & &4.31E-126 &2.17E-109 &2.81E-106 & & &25.071 &23.170 &22.804\\
		Recall & &4.20E-35 &3.37E-16 &4.36E-150 & & &12.522 &8.206 &27.650\\
		F1 score & &1.99E-64 &1.46E-88 &5.29E-155 & & &17.364 &20.634 &28.160\\[6pt]\midrule
		
		\multicolumn{10}{@{}l}{\textbf{Simulated environment} (300 samples)}\\[6pt]
		Precision & &3.20E-98 &1.69E-63 &2.65E-81 & & &25.611 &19.032 &22.427\\
		Recall & &8.12E-104 &2.56E-41 &1.57E-264 & & &26.665 &14.558 &62.541\\
		F1 score & &9.60E-134 &8.56E-99 &2.96E-242 & & &32.402 &25.719 &56.575\\[6pt] \midrule
		
		\multicolumn{10}{@{}l}{\textbf{PHM Real data - Simple uni-variate state} (900 samples)} \\[6pt]
		Precision & &2.27E-32 &7.29E-31 &9.95E-31 & & &12.082 &11.770 &11.742\\
		Recall & &1.27E-16 &1.55E-06 &8.19E-71 & & &8.357 &4.821 &18.607\\
		F1 score & &1.94E-19 &4.67E-34 &2.19E-67 & & &9.121 &12.423 &18.098\\ [6pt]\midrule
		
		\multicolumn{10}{@{}l}{\textbf{PHM Real data - Complex multi-variate state} (300 samples)}\\[6pt]
		Precision & &1.64E-60 &3.34E-54 &7.88E-59 & & &18.451 &17.207 &18.122\\
		Recall & &2.69E-10 &2.69E-02 &9.68E-01 & & &-6.425 &-2.219 &-0.041\\
		F1 score & &7.27E-01 &1.44E-08 &1.35E-03 & & &0.349 &5.748 &3.220\\
		\bottomrule
	\end{tabular}
	\caption{One-tail t-test - Ho: No difference in metrics. Ha: REINFORCE metric > Advanced algorithm metric}
	\label{tbl:ttest}
\end{table*}



\subsection{Training times}

%\begin{table*}[hbt!]\centering
%	\sffamily
%	\rowspace{1.3}
%	\begin{tabular}{@{}l r r r r@{}}
%		\arrayrulecolor{black!40}\toprule
%		\textbf{Environment} &\textbf{REINFORCE} &\textbf{A2C}&\textbf{DQN}&\textbf{PPO}\\ \midrule
%		Simulated  - No noise &214.23 &41.19&4.03&41.13\\
%		Simulated  - Low noise &199.89 &41.52&3.55&40.66\\
%		Simulated  - High noise &134.16 &17.88&1.53&20.90\\ \midrule
%		
%		PHM C01 SS - No noise &330.54 &18.85&2.08&32.65\\
%		PHM C01 SS - Low noise &426.79 &30.66&3.69&38.59\\
%		PHM C01 SS - High noise &333.13 &17.58&1.80&19.16\\ \midrule
%		
%		PHM C04 SS - No noise &299.31 &19.56&1.86&19.64\\
%		PHM C04 SS - Low noise &264.90 &18.27&2.00&19.69\\
%		PHM C04 SS - High noise &256.44 &17.65&1.58&19.11\\ \hdashline
%		
%		PHM C06 SS - No noise &339.65 &17.64&2.26&19.50\\
%		PHM C06 SS - Low noise &266.98 &19.33&1.84&19.19\\
%		PHM C06 SS - High noise &308.20 &34.21&4.18&30.94\\ \hdashline
%		
%		PHM C01 MS - No noise &655.21 &38.55&4.96&42.21\\
%		PHM C04 MS - No noise &615.58 &33.85&7.36&43.49\\
%		PHM C06 MS - No noise &625.37 &39.30&5.85&41.68\\  \midrule
%		
%		Overall average (s) &351.36 &27.07&3.24&29.90\\
%		\bottomrule
%\end{tabular}
%\caption{Training time for each model, across different environments. Averaged over 3 runs. Time is in seconds (s).}
%\label{tbl:TrainingTimes}
%\end{table*}

%\subsection{Algorithm timelines}
%\begin{itemize}
%	\item 1947: Monte Carlo Sampling
%	\item 1959: Temporal Difference Learning
%	\item 1989: Q-Learning
%	\item 1992: REINFORCE
%	\item 2013: DQN
%	\item 2016: A3C
%	\item 2017: PPO 
%\end{itemize}

\section{Environment description}

\subsection{simulated data}
\citep{davsic2006}. eq 16 from the paper 
parameters b0, b1, b2 and a under the form of a
complex power-exponential regression equation
power functions as approximating
functions of cutting tool wear, we proposed is powerexponential
function form

tool
flank wear VB
\[ VB = a \cdot t^{b_1} \cdot e^{b_2 \cdot t} \Big|_{t=t_0}^{t=t_1} \]

% \[ \evu{\frac{1}{5} \tan \theta}{\pi}{0} \]
b0	-2.4941	
b1	0.3342	
b2	0.03147	
a	0.08257	
Determination of Complex Power-
Exponential Regression Equation for
Functional Dependence between Tool Wear
and Cutting Time for
Cutting Speed v=79.2 [m/min]
Parameter calculation b0, b1 and b2 of the linear
regression for the mentioned example is consisted in
the solving of the normal system equation (12) with
the following shape:

%on the base of which is: b0=-2.4941; b1=0.3342;
%b2=0.03147 and a=0.08257.
%From that point the equation of the complex
%power-exponential regression equation has the shape:
%VB = 0.08257 â‹… t0.3342 â‹… e0.03147â‹…t


\textbf{Implementation details}: Normalization

init \\
reset \\
reward function \\

degradation model as exponential
\begin{equation}\label{eq:DegradationModel}
	H(t) = 1 - D_0 - {\rm e}^{(at^b)},
\end{equation}
where, $D_0$ is the initial degradation state while $a$ and $b$ are wear-rate coefficients that depend on the effect of temperature, vibration and other system stress parameters


\[
X(m,n) = \left\{\begin{array}{lr}
	x(n), & \text{for } 0\leq n\leq 1\\
	x(n-1), & \text{for } 0\leq n\leq 1\\
	x(n-1), & \text{for } 0\leq n\leq 1
\end{array}\right\} = xy
\]


\begin{equation}
R_t =
\begin{cases}
		\; 0, &  \quad \text{if}\; U(s_t) = U(s_{t+1}) = 0 \\
	\; -1.0, & \quad \text{if}\; U(s_t) = 0, U(s_{t+1}) = 1 \\
	\; R_{ff}, & \quad \text{if}\; U(s_t) = 1\\
\end{cases}
\label{eq:RewardFunction}
\end{equation}

\subsection{SS}
\begin{verbatim}
	def _get_observation(self, index):
	next_state = np.array([
	self.df['time'][index],
	self.df['tool_wear'][index]
	], dtype=np.float32)
	
	return next_state
\end{verbatim}

\subsection{MS}
\begin{verbatim}
	def _get_observation(self, index):
		next_state = np.array([
		self.df['tool_wear'][index],
		self.df['force_x'][index],
		self.df['force_y'][index],
		self.df['force_z'][index],
		self.df['vibration_x'][index],
		self.df['vibration_y'][index],
		self.df['vibration_z'][index],
		self.df['acoustic_emission_rms'][index]
		], dtype=np.float32)
	
	return next_state
\end{verbatim}

\begin{verbatim}
	# Machine data frame properties
	self.df = df
	self.df_length = len(self.df.index)
	self.df_index = 0
	
	# Milling operation and tool parameters
	self.wear_threshold = wear_threshold
	self.max_operations = max_operations
	self.breakdown_chance = breakdown_chance
	self.add_noise = add_noise
	self.R1 = R1
	self.R2 = R2
	self.R3 = R3
	self.reward = 0.0
\end{verbatim}

\subsection{main env}
\begin{verbatim}
	env = MillingTool_SS_NT(df_train, WEAR_THRESHOLD_NORMALIZED, MILLING_OPERATIONS_MAX, ADD_NOISE, BREAKDOWN_CHANCE, R1, R2, R3)
\end{verbatim}

\subsection{test env}
\begin{verbatim}
	env_test = MillingTool_SS_NT(df_test, WEAR_THRESHOLD_ORG_NORMALIZED, MILLING_OPERATIONS_MAX, ADD_NOISE, BREAKDOWN_CHANCE, R1, R2, R3)
\end{verbatim}

\section{The core elements of the 4 algorithms}
The core elements of the 4 algorithms. \citep{graesser2019} \\

\textbf{REINFORCE}: Paper \citep{REINFORCE-williams1992}
\begin{equation}
	\nabla_ \theta J(\pi_\theta) = \mathbb{E}_t \; [ \; R_t(\tau) \; \nabla_\theta \ln \pi_\theta(a_t \vert s_t) \;]
	\label{eq:REINFORCE}
\end{equation}

\textbf{DQN}:
Learning Q-function in DQN. Paper \citep{DQN-mnih2013}. Implementation notes: "\textcolor{red}{*SB3 says:*} Deep Q Network (DQN) builds on Fitted Q-Iteration (FQI) and make use of different tricks to stabilize the learning with neural networks: it uses a replay buffer, a target network and gradient clipping" 
\begin{equation}
	Q^\pi (s, a) = r + \gamma  \max_{a'} Q^\pi (s', a')
	\label{eq:DQN}
\end{equation}

\textbf{A2C}: Actors learn parameterized policy $\pi_{\theta}$ using the policy-gradient as shown in Equation \ref{eq:A2C}. "\textcolor{red}{*SB3 says:*} A synchronous, deterministic variant of Asynchronous Advantage Actor Critic (A3C). It uses multiple workers to avoid the use of a replay buffer.". Paper \citep{A2C-mnih2016} 

\begin{equation}
	\nabla_ \theta J(\pi_\theta) = \mathbb{E}_t \; [ \; A^\pi_t \; \nabla_\theta \ln \pi_\theta(a_t \vert s_t) \;]
	\label{eq:A2C}
\end{equation}

Where the advantage function $A^\pi_t (s_t, a_t)$ measures how good or bad the action is w.r.t. policy's average, for a particular state using Equation \ref{eq:A2CAF}

\begin{equation}
	A^\pi_t (s_t, a_t) = Q^\pi (s_t, a_t) - V^\pi (s_t)
	\label{eq:A2CAF}
\end{equation}

\textbf{PPO}: " \textcolor{red}{*SB3 says:*} The Proximal Policy Optimization algorithm combines ideas from A2C (having multiple workers) and TRPO (it uses a trust region to improve the actor). The main idea is that after an update, the new policy should be not too far from the old policy. For that, ppo uses clipping to avoid too large update.". Paper \citep{PPO-schulman2017}


\begin{equation}
	J^{CLIP} (\theta) = \mathbb{E}_t \; [ \; min (r_t(\theta) A^{\pi_{\theta_{old}}}_t, clip(r_t(\theta), 1-\epsilon, 1+\epsilon) A^{\pi_{\theta_{old}}}_t)]
	\label{eq:PPO}
\end{equation}

\section{The REINFORCE algorithm}
Three key features of any RL algorithm:
\begin{enumerate}
	\item Policy: $\pi_\theta$ = Probablities of all actions, given a state. Parameterized by $\theta$
	\item Objective function:
	\begin{equation}
		\max_{\theta} J(\pi_{\theta}) = \mathop{\mathbb{E}}_{\tau \sim \pi_\theta} [R(\tau)]
	\end{equation}
	\item Method: Way to udate the parameters = Policy Gradient

\end{enumerate}

\subsection{Policy gradient numerical computation}

\begin{enumerate}
	\item Plain vanilla: 
	\begin{equation}
		\nabla_ \theta J(\pi_\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \; [ \; \sum_{t=0}^T R_t(\tau) \; \nabla_\theta \ln \pi_\theta(a_t \vert s_t) \;]
	\end{equation}
	\item With Monte Carlo sampling and approximation: $\nabla_\theta J(\pi_\theta) \approx [ \; \sum_{t=0}^T R_t(\tau) \; \nabla_\theta \ln \pi_\theta(a_t \vert s_t) \;]$
	\item With baseline: $\nabla_\theta J(\theta) \approx [ \; \sum_{t=0}^T (R_t(\tau) - b(s_t)) \; \nabla_\theta \ln \pi_\theta(a_t \vert s_t) \;]$
	\item Where, baseline does not change per time-step, it is for the entire trajectory
	\item One baseline option: $V^\pi$ - leads to Actor-Critic algorithm
	\item Simpler option: Average returns over trajectory: $b = \frac{1}{T}\sum_{t=0}^T R_t(\tau) $
\end{enumerate}

Algorithm
%1. Initialize $\alpha$, $\gamma$ and $\theta$ i.e. weigths of the NN
%2. for episodes = 0 to MAX_EPISODES:
%- sample trajectory $\tau$
%- set $\nabla_\theta J(\pi_{\theta})$ = 0
%- for t=0 to T:
%- $R_t(\tau) = \sum_{t'=t}^{T} \gamma^{t'-t} r'_t$
%- $\nabla_\theta J(\pi_\theta)  = \mathbb{E}_{\tau \sim \pi_\theta} \; [ \; \sum_{t=0}^T R_t(\tau) \; \nabla_\theta \ln \pi_\theta(a_t \vert s_t) \;]$
%- end for sampled trajectory
%- $\theta = \theta + \alpha \nabla_\theta J(\pi_\theta) $
%3. end for all episodes 
%
%Implementation notes:
%1. Code is inspired by Laura Graesser's (LG, 2020 book) implementation, however with large modifications
%2. We use the concept of Agent (instead of 'pi' or '$policy_pi$') and a separate network class (i.e. how the function approximator is implemented, it could well be a linear regression)
%3. **Important concept**: "Loss" in the implementation below, is the "objective" $J$. In our algorithm, we want to **maximize** it. PyTorch's optimizer, by default, MINIMIZES it (as it is called "loss"), we therefore add "-" negate it, so as to maximize it. 
%4. Also in the final plot, notice "loss" is RISING and follows the rewards, which is expected as "loss" is really being maximized.  
%5. self.pd.probs = prob. distribution of all actions
%6. Sum of all possible action probabilities = 1.0
%7. Note that episodes > 300 start showing repeated patterns. Rewards drop and rise in cylces. 300 is ideal and hence suggested in LG (2020)



\section{Discussion}

\subsection{Precision or Recall?}
- Precision => low FP => False replacement-action reduced. \textcolor{red}{***} Unnecessary tool replacement reduced. Tool life maximized, down time minimized, production disruption minimized \\
- Recall => low FN => False declaration of normal operation reduced. Reduce missed replacements. Tool replacements increased. \textcolor{red}{***} Product quality not compromised. 


limitiongs of the slecting model for evaluation

\cite{SB3-paper} -- quote from paper - intro section -- "A major challenge is that small implementation details can have a substantial effect on performance often greater than the difference between algorithms (Engstrom et al., 2020). It is particularly important that  implementations used as experimental baselines are reliable; otherwise, novel algorithms compared to weak baselines lead to inated estimates of performance improvements"

DPO paper - Rafael Rafailov May-2023 --- Direct Preference Optimization: Your Language Model is Secretly a Reward Model 

5.2 Instability of Actor-Critic Algorithms
We can also use our framework to diagnose instabilities with standard actor-critic algorithms used
for the RLHF, such as PPO. We follow the RLHF pipeline and focus on the RL fine-tuning step
outlined in Section 3. We can draw connections to the control as inference framework [18] for the
constrained RL problem outlined in 3. We assume a parameterized model and minimize
This is the same objective optimized in prior works [45, 35, 1, 23] using the DPO-equivalent reward
for the reward class of In this setting, we can interpret the normalization term in 
as the soft value function of the reference policy . While this term does not affect the optimal
solution, without it, the policy gradient of the objective could have high variance, making learning
unstable. We can accommodate for the normalization term using a learned value function, but that
can also be difficult to optimize. Alternatively, prior works have normalized rewards using a human
completion baseline, essentially a single sample Monte-Carlo estimate of the normalizing term. In
contrast the DPO reparameterization yields a reward function that does not require any baselines.

\section{Conclusion}


%% References
\bibliography{ES_bibliography}
\end{document}
