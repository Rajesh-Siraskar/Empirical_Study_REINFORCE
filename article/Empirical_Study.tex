%% (2) Build options: PdfLaTex + BibTex
\documentclass[a4paper, 12pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[round]{natbib}
\bibliographystyle{plainnat}

\usepackage[pass]{geometry}
\usepackage{graphicx}
\graphicspath{{./images/}} % Images folder

%\usepackage{csquotes} 	% Quotes
\usepackage[table]{xcolor} %
\usepackage{booktabs}	% Tables
\usepackage{arydshln} 	% dashed lines in tables
\usepackage{pdflscape}	% Landscape page

\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsfonts}
\usepackage{amsmath,amssymb}
\usepackage{textcomp}	% for \texttrademark
\usepackage{setspace}
\usepackage[spaces, hyphens]{url}
\usepackage[colorlinks, allcolors=blue]{hyperref} 

% Macros
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcommand{\rowspace}[1]{\renewcommand{\arraystretch}{#1}}
\renewenvironment{abstract}
{\small
	\begin{center}
		\bfseries \abstractname\vspace{-.5em}\vspace{0pt}
	\end{center}
	\list{}{
		\setlength{\leftmargin}{.25cm}%
		\setlength{\rightmargin}{\leftmargin}%
	}%
	\item\relax}
{\endlist}

%% Quotes with epigraph style
\usepackage{epigraph}
% \epigraphsize{\small}% Default
\setlength\epigraphwidth{12cm}
\setlength\epigraphrule{0pt}
\usepackage{etoolbox}
\makeatletter
\patchcmd{\epigraph}{\@epitext{#1}}{\itshape\@epitext{#1}}{}{}
\makeatother
%% End Quotes macro

\title{An empirical study of the na\"ive REINFORCE algorithm for predictive maintenance of industrial milling machines}
\author{Rajesh Siraskar}

\onehalfspacing

\begin{document}
\maketitle

\begin{abstract}
In this empirical study, we document the performance of a simple, early reinforcement learning (RL) algorithm, REINFORCE, implemented for a predictive maintenance problem -- an optimal tool replacement policy for a milling machine. We compare a na\"ive implementation of REINFORCE against the policies of industry-grade implementations (Stable-Baselines3) of three advanced algorithms, namely, Deep Q-Network (DQN), Advantage Actor-Critic (A2C) and Proximal Policy Optimization (PPO). This work is aimed at industrial practitioners not accustomed to the complex hyperparameter tuning often required to get RL to work. Our broad goal was to understand the performance of \textit{untuned} algorithms under various scenarios: (1) simulation tool-wear data (2) real tool-wear data (benchmark IEEE NUAA Ideahouse dataset) (3) added noise levels and a random chance of break-down. 

Model performance was measured by how accurately the predictive maintenance agent suggested tool replacement when compared to a deterministic preventive maintenance rule based on the tool-wear threshold. Across variants of the environment, REINFORCE models demonstrated a tool replacement precision of 0.687 against 0.449 for A2C, 0.418 for DQN, and 0.472 for PPO. The F1 scores were 0.609, 0.442, 0.374 and 0.345 respectively. Variability in precision and F1 was lower for REINFORCE by 0.08 and 0.016, when compared to the average of the three advanced algorithms. Comparing the \textit{best model} over 10 rounds of training produced surprisingly larger gaps in performance. REINFORCE precision/F1 stood at 0.884/0.873. The best A2C, DQN and PPO models produced 0.520/0.639, 0.651/0.740 and 0.558/0.580 respectively. Our findings indicate that the computationally lightweight REINFORCE performs significantly well for this particular problem. Consequently, for this particular problem, selecting the na\"ive REINFORCE could be a more suitable and effective policy generating alternative to more advanced complex algorithms. 

For reproducibility, model training and testing code, data and the trained REINFORCE models have been uploaded to \href{https://github.com/Rajesh-Siraskar/Empirical-Study\_REINFORCE-for-predictive-maintenance}{https://github.com/Link} 
\end{abstract}

\noindent \textbf{Keywords}: Predictive maintenance, milling machines, Reinforcement Learning, REINFORCE

\section*{Abbreviations}

\begin{table*}[!htbp]\centering
	\sffamily
	\rowspace{1.3}
	\begin{tabular}{L{1cm} L{4.5cm} L{1cm} L{4.5cm}}
		\arrayrulecolor{black!40}\toprule	
		
		DQN & Deep Q-Network & A2C & Advantage Actor-Critic\\
		PPO & Proximal Policy Optimization & RF & REINFORCE\\
		SS & Single-variable state & MS & Multi-variate state\\
		TP &True positive &TN &True negative\\
		FP &False positive &FN &False negative\\
		RL & Reinforcement Learning & SB3 & Stable-Baselines3\\
		PHM & The Prognostics and Health Management Society & \textcolor{red}{*AA*} & \textcolor{red}{Advanced Algorithms} \\

		\bottomrule
	\end{tabular}
	\label{tbl:abbrev}
\end{table*}

\newpage
\thispagestyle{empty}
\listoffigures
\listoftables

\section{Introduction}
% $$$ Quote
%\epigraph{"Plurality should not be posited without necessity" -- Of two competing theories, the simpler explanation of an entity is to be preferred}{--- \textup{William of Ockham (1285â€“1347)}, The Occams razor principle}

%\noindent Milling machines are highly versatile, ubiquitous tools serving a variety of industries. 
% $$$ End Quote and first line of para without indent

Milling machines are highly versatile, ubiquitous tools serving a variety of industries. A milling machine removes metal from the work piece by rotating and driving a cutting device into it. Abrasive forces cause tool wear, and optimal tool replacement reduces direct costs and optimizes the machines' downtime. With the 2023 milling machine market valued at USD 68.3 billion \citep{milling-market}, this is an important goal for the industry. The cutting tool experiences multiple types of wear as it cuts through metal. Tool wear depends in several factors such as the cutting speed, force applied to the tool, lubrication and materials of the work piece and cutting tool. 

Reinforcement learning (RL) is an artificial intelligence technique inspired by nature. Fig. \ref{fig:RL-loop} \citep{barto2018} shows the RL learning feedback loop. An actor or ``agent" interacts with an environment and learns via ``trial-and-error". It acts based on stimuli or feedback received from the environment after performing a certain action. Actions that help in achieving the learning goal receive a reward while actions that do not, are punished. Repeating this loop over thousands of episodes, good actions are ``reinforced", thereby building a ``policy" that is optimized for that goal. In the case of predictive maintenance for milling machines, the agent is the ``planner" with a goal of learning an optimal tool replacement policy. The environment consists of sensors attached to the machine and related information such as job specifications, environment conditions etc.

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.5\textwidth]{RL-loop.pdf}
	\caption{Reinforcement Learning}
	\label{fig:RL-loop}
\end{figure}

Introduced in 1992, the REINFORCE algorithm \citep{REINFORCE-williams1992} is considered as a basic reinforcement learning algorithm. It is a policy-based, on-policy algorithm, capable of handling both discrete and continuous observation and action domains. In practice the REINFORCE algorithm is considered as a ``weak" algorithm and superseded by several algorithms developed since. Most notably the Q-Learning and its deep-neural network version, the DQN \citep{DQN-mnih2013}, followed by Actor-Critic \citep{A2C-mnih2016} and one of the most robust modern-day algorithms, the PPO \citep{PPO-schulman2017}.

While most studies on RL algorithms are evaluated on Open AI Gym environments, our experiments cover the predictive maintenance problem using a custom built environment. Also, our study focuses on the earliest RL algorithm, the REINFORCE. This is often neglected by studies, as it is considered an early and weak algorithm. In the industry the milling tool is replaced after a set threshold. We use this ``deterministic preventive maintenance" policy as the baseline for comparing the RL algorithm policy. 

Our systematic evaluation, based on levels of environment difficulty, different bench-mark data sets and varying noise levels allow a broader, more robust, comparison of the algorithms. Finally, we conduct statistical tests to ensure a robust statistical-evidence based conclusion. Based on experiments we show that REINFORCE works surprisingly well on tool-replacement precision and F1-beta (0.5), over all the variants. The recall and F1-score are better or at-par with the advanced algorithms.

The main \textbf{contributions} of this research are (1) evaluation on a custom designed environment built for the predictive maintenance problem (2) targeted toward the industrial practitioner not accustomed to complex hyperparameter tuning and (3) use of simple performance evaluation statistical measures and plots that industrial practitioners are normally used to.

The rest of the paper is structured as follows: In the next section we survey some related work. Section \ref{sec:Background} provides the necessary technical background describing the algorithms studied in this research and describe the implementation details of the REINFORCE algorithm and the predictive maintenance environment. Section \ref{sec:Method} discusses the methodology adopted for training, testing and evaluation. Section \ref{sec:Results} presents the results of the experiments, that are discussed in Section \ref{sec:Discussion}. Finally, we summarize and draw conclusions in Section \ref{sec:Conclusion}.

\section{Related Work}\label{sec:SLR}
Significant work has been conducted in the application of RL for predictive maintenance \citep{Panzer2021, Erhan2021, siraskar2023}, in general. However, we did not find\footnote{Search terms: \texttt{``reinforcement learning AND tool wear AND maintenance"}. As of: 10-Jul-2023.} any work done for predictive maintenance of \textit{milling machines} across three major journal index services\footnote{IEEE Xplore\texttrademark{}, Scopus\texttrademark{} and Web Of Science\texttrademark{}}. % $$$ Note: 1 article returned (\cite{dai2021reinforcement}) is NOT predictive maintenance - but clamping position optimization. 
While RL is not, traditional machine learning methods \textit{have} been applied; for example \cite{Qin2023} and \cite{Qiang2023} apply tool-wear law and physics based models before applying ML. \cite{Twardowski2023} and \cite{Denkena2023} use data gathered by sensors. While \cite{Twardowski2023} use two different classification trees, \cite{Denkena2023} use data recorded on other similar machines for building ML models. \cite{oshida2023development} proposes real-time tool wear detection using a stacked LSTM\footnote{long short-term memory networks} encoder-decoder model for anomaly detection as a mechanism to address predictive maintenance.  

Research work focusing of comparing experimental and empirical analysis of various RL algorithms, has been limited to using standard ``benchmark" OpenAI Gym environments. \cite{sandeep2022experimental} documents experimental evaluation of four policy-gradient and actor-critic algorithms PPO, SAC, DDPG and A2C using OpenAI Gym environments such as the Pendulum, Mountain Car, Bipedal Walker, Lunar Landing and Atari 2600 game environments. \cite{Krishna2020} evaluate DQN, DoubleDQN, A2C, REINFORCE and PPO using Cartpole, Space Invaders and the Lunar Lander. \cite{dulac2021, dulac2020empirical} are significant contributions toward analyzing empirical studies directed toward \textit{real-world} challenges. They apply real-world design concepts on the Cartpole and other complex environments such as humanoid and walkers from the Real-World Reinforcement Learning (RWRL) Suite\footnote{\href{https://github.com/google-research/realworldrl_suite}{RWRL link}}. These environments are then evaluated for multiple RL algorithms such as, REINFORCE, Trust Region Policy Optimization (TRPO) and Deep Deterministic Policy Gradient (DDPG). 

\cite{dulac2021, henderson2018deep} tackle RL for continuous control. \cite{henderson2018deep} evaluate DDPG, ACKTR, TRPO and PPO on complex MuJoCo\footnote{\href{https://mujoco.org/}{MuJoCo are environments for studying ``Multi-Joint dynamics with Contact"}} environments such as the HalfCheetah, Hopper, Walker and Swimmer. As in our research they used the OpenAI baseline implementations of RL algorithms for the experiments and evaluation. \cite{ford2022cognitive} is experimental evaluation we found that was based on real-world application. They compare DQN, A2C and PPO for choosing the operational radio frequency (RF) mode for a multi-function RF system and go on to suggest that PPO is the best.

The survey shows that most existing work use standard OpenAI Gym environments, which although necessary for bench marking performance, do not provide coverage of industrial predictive maintenance. We attempt to bridge this gap using a custom built environment, explained in the next section. 

\section{Technical Background}\label{sec:Background}
\subsection{Algorithms}
\textbf{Algorithm timelines}
\begin{itemize}
	\item 1947: Monte Carlo Sampling
	\item 1959: Temporal Difference Learning
	\item 1989: Q-Learning
	\item 1992: REINFORCE
	\item 2013: DQN
	\item 2016: A3C
	\item 2017: PPO 
\end{itemize}

\subsubsection{The core elements of the 4 algorithms}
The core elements of the 4 algorithms. \citep{graesser2019} \\

\textbf{REINFORCE}: Paper \citep{REINFORCE-williams1992}
\begin{equation}
	\nabla_ \theta J(\pi_\theta) = \mathbb{E}_t \; [ \; R_t(\tau) \; \nabla_\theta \ln \pi_\theta(a_t \vert s_t) \;]
	\label{eq:REINFORCE}
\end{equation}

\textbf{DQN}:
Learning Q-function in DQN. Paper \citep{DQN-mnih2013}. Implementation notes: "\textcolor{red}{*SB3 says:*} Deep Q Network (DQN) builds on Fitted Q-Iteration (FQI) and make use of different tricks to stabilize the learning with neural networks: it uses a replay buffer, a target network and gradient clipping" 
\begin{equation}
	Q^\pi (s, a) = r + \gamma  \max_{a'} Q^\pi (s', a')
	\label{eq:DQN}
\end{equation}
\textbf{DQN}
Deep Q Network (DQN) builds on Fitted Q-Iteration (FQI) \citep{riedmiller2005neural} and make use of different tricks to stabilize the learning with neural networks: it uses a replay buffer, a target network and gradient clipping.

\textbf{A2C}: Actors learn parameterized policy $\pi_{\theta}$ using the policy-gradient as shown in Equation \ref{eq:A2C}. "\textcolor{red}{*SB3 says:*} A synchronous, deterministic variant of Asynchronous Advantage Actor Critic (A3C). It uses multiple workers to avoid the use of a replay buffer.". Paper \citep{A2C-mnih2016} 

\begin{equation}
	\nabla_ \theta J(\pi_\theta) = \mathbb{E}_t \; [ \; A^\pi_t \; \nabla_\theta \ln \pi_\theta(a_t \vert s_t) \;]
	\label{eq:A2C}
\end{equation}

Where the advantage function $A^\pi_t (s_t, a_t)$ measures how good or bad the action is w.r.t. policy's average, for a particular state using Equation \ref{eq:A2CAF}

\begin{equation}
	A^\pi_t (s_t, a_t) = Q^\pi (s_t, a_t) - V^\pi (s_t)
	\label{eq:A2CAF}
\end{equation}

\textbf{PPO}: " \textcolor{red}{*SB3 says:*} The Proximal Policy Optimization algorithm combines ideas from A2C (having multiple workers) and TRPO (it uses a trust region to improve the actor). The main idea is that after an update, the new policy should be not too far from the old policy. For that, ppo uses clipping to avoid too large update.". Paper \citep{PPO-schulman2017}


\begin{equation}
	J^{CLIP} (\theta) = \mathbb{E}_t \; [ \; min (r_t(\theta) A^{\pi_{\theta_{old}}}_t, clip(r_t(\theta), 1-\epsilon, 1+\epsilon) A^{\pi_{\theta_{old}}}_t)]
	\label{eq:PPO}
\end{equation}

\subsubsection{The REINFORCE algorithm}
Three key features of any RL algorithm:
\begin{enumerate}
	\item Policy: $\pi_\theta$ = Probablities of all actions, given a state. Parameterized by $\theta$
	\item Objective function:
	\begin{equation}
		\max_{\theta} J(\pi_{\theta}) = \mathop{\mathbb{E}}_{\tau \sim \pi_\theta} [R(\tau)]
	\end{equation}
	\item Method: Way to udate the parameters = Policy Gradient
\end{enumerate}
sB-3 dOES NOT IMPLEMENT THE REINFORCE	 --- https://stable-baselines3.readthedocs.io/en/master/guide/algos.html

Model Free --> Policy Gradient/Actor-Critic --> REINFORCE)
REINFORCE (Monte-Carlo policy gradient) 
IEEE NUAA Ideahouse dataset has been used in this paper for the RUL estimation of the milling cutter. \cite{NUAA-data-set} Tool wear modeling is the first step to assist in predicting. 


\subsubsection{Policy gradient numerical computation}

\begin{enumerate}
	\item Plain vanilla: 
	\begin{equation}
		\nabla_ \theta J(\pi_\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \; [ \; \sum_{t=0}^T R_t(\tau) \; \nabla_\theta \ln \pi_\theta(a_t \vert s_t) \;]
	\end{equation}
	\item With Monte Carlo sampling and approximation: $\nabla_\theta J(\pi_\theta) \approx [ \; \sum_{t=0}^T R_t(\tau) \; \nabla_\theta \ln \pi_\theta(a_t \vert s_t) \;]$
	\item With baseline: $\nabla_\theta J(\theta) \approx [ \; \sum_{t=0}^T (R_t(\tau) - b(s_t)) \; \nabla_\theta \ln \pi_\theta(a_t \vert s_t) \;]$
	\item Where, baseline does not change per time-step, it is for the entire trajectory
	\item One baseline option: $V^\pi$ - leads to Actor-Critic algorithm
	\item Simpler option: Average returns over trajectory: $b = \frac{1}{T}\sum_{t=0}^T R_t(\tau) $
\end{enumerate}

Algorithm
%1. Initialize $\alpha$, $\gamma$ and $\theta$ i.e. weigths of the NN
%2. for episodes = 0 to MAX_EPISODES:
%- sample trajectory $\tau$
%- set $\nabla_\theta J(\pi_{\theta})$ = 0
%- for t=0 to T:
%- $R_t(\tau) = \sum_{t'=t}^{T} \gamma^{t'-t} r'_t$
%- $\nabla_\theta J(\pi_\theta)  = \mathbb{E}_{\tau \sim \pi_\theta} \; [ \; \sum_{t=0}^T R_t(\tau) \; \nabla_\theta \ln \pi_\theta(a_t \vert s_t) \;]$
%- end for sampled trajectory
%- $\theta = \theta + \alpha \nabla_\theta J(\pi_\theta) $
%3. end for all episodes 
%
%Implementation notes:
%1. Code is inspired by Laura Graesser's (LG, 2020 book) implementation, however with large modifications
%2. We use the concept of Agent (instead of 'pi' or '$policy_pi$') and a separate network class (i.e. how the function approximator is implemented, it could well be a linear regression)
%3. **Important concept**: "Loss" in the implementation below, is the "objective" $J$. In our algorithm, we want to **maximize** it. PyTorch's optimizer, by default, MINIMIZES it (as it is called "loss"), we therefore add "-" negate it, so as to maximize it. 
%4. Also in the final plot, notice "loss" is RISING and follows the rewards, which is expected as "loss" is really being maximized.  
%5. self.pd.probs = prob. distribution of all actions
%6. Sum of all possible action probabilities = 1.0
%7. Note that episodes > 300 start showing repeated patterns. Rewards drop and rise in cylces. 300 is ideal and hence suggested in LG (2020)




\subsubsection{Stable-Baselines3}
Stable-Baselines3 \cite{SB3-paper}, is Open Source and very popular among the RL community\footnote{As of 27-Jun-2023, it had 6k+ stars and 424 closed pull requests}. Stable-Baselines3 was initially based on the Open AI baselines \citep{OpenAI-baselines} and is completely rewritten using PyTorch. 

 for  , implementation s  the advanced algorithms -- DQN, A2C and PPO and compare its performance to a na\"ive custom implementation of the REINFORCE algorithm. We use Stable-Baselines3 (SB3), the highly popular and reliable implementations of DQN, A2C and PPO. As of 27-Jun-2023, the REINFORCE was not implemented by Stable-Baselines and we therefore custom implemented a basic version. 





\subsection{Implementation details}
xx\\

\subsubsection{Data description}
- Simulated\\
- PHM\\
\subsubsection{Environment design}
we follow the procedure in \cite{graesser2019} pg 289 Part iv, ch14-15.
\subsubsection{simulated data}
\citep{davsic2006}. eq 16 from the paper 
parameters b0, b1, b2 and a under the form of a
complex power-exponential regression equation
power functions as approximating
functions of cutting tool wear, we proposed is powerexponential
function form

tool
flank wear VB

\begin{equation}
	VB = a \cdot t^{b_1} \cdot e^{b_2 \cdot t} \Big|_{t=t_0}^{t=t_1}
	\label{eq:Dasic}
\end{equation}


% \[ \evu{\frac{1}{5} \tan \theta}{\pi}{0} \]
b0	-2.4941	
b1	0.3342	
b2	0.03147	
a	0.08257	
Determination of Complex Power-
Exponential Regression Equation for
Functional Dependence between Tool Wear
and Cutting Time for
Cutting Speed v=79.2 [m/min]
Parameter calculation b0, b1 and b2 of the linear
regression for the mentioned example is consisted in
the solving of the normal system equation (12) with
the following shape:

%on the base of which is: b0=-2.4941; b1=0.3342;
%b2=0.03147 and a=0.08257.
%From that point the equation of the complex
%power-exponential regression equation has the shape:
%VB = 0.08257 â‹… t0.3342 â‹… e0.03147â‹…t


\textbf{Implementation details}: Normalization

init \\
reset \\
reward function \\

degradation model as exponential
\begin{equation}\label{eq:DegradationModel}
	H(t) = 1 - D_0 - {\rm e}^{(at^b)},
\end{equation}
where, $D_0$ is the initial degradation state while $a$ and $b$ are wear-rate coefficients that depend on the effect of temperature, vibration and other system stress parameters.

\begin{equation}
	R_t =
	\begin{cases}
		\; 0, &  \quad \text{if}\; U(s_t) = U(s_{t+1}) = 0 \\
		\; -1.0, & \quad \text{if}\; U(s_t) = 0, U(s_{t+1}) = 1 \\
		\; R_{ff}, & \quad \text{if}\; U(s_t) = 1\\
	\end{cases}
	\label{eq:RewardFunction}
\end{equation}

\subsubsection{SS}
\begin{verbatim}
	def _get_observation(self, index):
	next_state = np.array([
	self.df['time'][index],
	self.df['tool_wear'][index]
	], dtype=np.float32)
	
	return next_state
\end{verbatim}

\subsubsection{MS}
\begin{verbatim}
	def _get_observation(self, index):
	next_state = np.array([
	self.df['tool_wear'][index],
	self.df['force_x'][index],
	self.df['force_y'][index],
	self.df['force_z'][index],
	self.df['vibration_x'][index],
	self.df['vibration_y'][index],
	self.df['vibration_z'][index],
	self.df['acoustic_emission_rms'][index]
	], dtype=np.float32)
	
	return next_state
\end{verbatim}

\begin{verbatim}
	# Machine data frame properties
	self.df = df
	self.df_length = len(self.df.index)
	self.df_index = 0
	
	# Milling operation and tool parameters
	self.wear_threshold = wear_threshold
	self.max_operations = max_operations
	self.breakdown_chance = breakdown_chance
	self.add_noise = add_noise
	self.R1 = R1
	self.R2 = R2
	self.R3 = R3
	self.reward = 0.0
\end{verbatim}

\subsubsection{main env}
\begin{verbatim}
	env = MillingTool_SS_NT(df_train, WEAR_THRESHOLD_NORMALIZED, MILLING_OPERATIONS_MAX, ADD_NOISE, BREAKDOWN_CHANCE, R1, R2, R3)
\end{verbatim}

\subsubsection{test env}
\begin{verbatim}
	env_test = MillingTool_SS_NT(df_test, WEAR_THRESHOLD_ORG_NORMALIZED, MILLING_OPERATIONS_MAX, ADD_NOISE, BREAKDOWN_CHANCE, R1, R2, R3)
\end{verbatim}



- SS\\
- MS\\
- States: We normalize the tool wear and other state features, $x \in [0,\;1] \subset \mathbb{R} $. This allows for adding white noise of similar magnitudes across experiments of different data-sets

- Actions\\
- Reward signal\\
\begin{itemize}
	\item R1 = +1 
	\item R2 = -1
	\item R3 = -100 => higher neg. Improve recall. Lower neg. Improve precision. 
	\item Hyper-parameters for Precision or Recall control: 
	\item Precision => low FP => False replacement-action reduced. \textcolor{red}{***} Unnecessary tool replacement reduced. Tool life maximized, down time minimized, production disruption minimized. 
	\item - Recall => low FN => False declaration of normal operation reduced. Reduce missed replacements. Tool replacements increased. \textcolor{red}{***} Product quality not compromised. 
	\item \textbf{LOOK AHEAD PARAM}: 
\end{itemize}

- Transition function \citep{graesser2019}: pg333 "Transition function  - also known as model. can be learnt or programmed programmable rules are common and lead to increasing complexity. \textcolor{red}{***} We use a hybrid model - partly learnt and partly programmed": \textbf{Noise and breakdown}\\

\subsubsection{Network architecture and basic hyper-parameters}

\begin{table*}\centering
	\sffamily
	\rowspace{1.5}
	\begin{tabular}{L{2cm} R{2.5cm} R{2.5cm} R{2.5cm} R{3cm}}
		\arrayrulecolor{black!40}\toprule
		&\textbf{A2C}&\textbf{DQN}&\textbf{PPO}&\textbf{REINFORCE}\\ \midrule
		
		Network\par architecture&input dim x\par [64|Tanh x 64|Tanh]\par x output dim&input dim x\par [64|Tanh x 64|Tanh]\par x output dim&input dim x\par [64|Tanh x 64|Tanh]\par x output dim&input dim x\par [64|ReLU]\par x output dim\\
		Layers&2&2&2&1\\
		Units&64  x 64&64  x 64&64  x 64&64\\
		Activation&Tanh, Tanh&Tanh, Tanh&Tanh, Tanh&ReLU\\
		Optimizer&RMSprop&Adam&Adam&Adam\\ \midrule
		Learning rate&0.0007&0.0001&0.0003&0.01\\
		Gamma&0.99&0.99&0.99&0.99\\			
		\bottomrule
	\end{tabular}
	\caption{Comparing the network architecture and basic hyper-parameters across algorithms}
	\label{tbl:network-architecture}
\end{table*}

\subsubsection{PPO hyperparms}
- soure of ppo implementation details \url{https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/}\\
- source of SB3 network mp : \url{https://github.com/openai/baselines/blob/ea25b9e8b234e6ee1bca43083f8f3cf974143998/baselines/common/models.py#L75-L103}\\
implementation guide source >> https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/

By default, PPO uses a simple MLP network consisting of two layers of 64 neurons and Hyperbolic Tangent as the activation function. Then PPO builds a policy head and value head that share the outputs of the MLP network. Below is a pseudocode:
%\textt{
	%network = Sequential(
	%layer_init(Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
	%Tanh(),
	%layer_init(Linear(64, 64)),
	%Tanh(),
	%)
	%value_head = layer_init(Linear(64, 1), std=1.0)
	%policy_head = layer_init(Linear(64, envs.single_action_space.n), std=0.01)
	%hidden = network(observation)
	%value = value_head(hidden)
	%action = Categorical(policy_head(hidden)).sample()}

\subsubsection{dqn hyperparms}


default hyperparms : \url{https://stable-baselines3.readthedocs.io/en/master/_modules/stable_baselines3/common/policies.html}

overridden in indiv policies for example 
SB3 DQN hypoerparms for example were taken from ;
Paper: https://arxiv.org/abs/1312.5602, https://www.nature.com/articles/nature14236
Default hyperparameters are taken from the Nature paper,
except for the optimizer and learning rate that were taken from Stable Baselines defaults



\subsubsection{Hyper-parameters for Precision or Recall control}
- xxx\\




\section{Methodology}\label{sec:Method}

\textit{The methodology explains in detail what the researcher did to undertake the research. Various aspects of the research have to be outlined: The overall structure and operation of the experiment or observational experience. The groups studied in the research including the size of each group and any features of the subjects which may be relevant to the topic being researched. The variables that were changed between groups and the variables measured as a result of the changes. The conditions under which the research was undertaken and any factors or variations in conditions which may have an impact on the results. The methods of data analysis used in order to analyse and collate the results. \textcolor{red}{***}Any limitations of the data collected.}

\subsection{Training}
 \textbf{training}
 \begin{itemize}
 	\item Training: SB3 - 10 k eps. 3 times. Average their outputs
 	\item Testing: 
 	\begin{itemize}
 		\item Avg. over 5 rounds.
 		\item Each round - avg over 40 test cases x 10 test rounds
 		\item Total: 40 x 10 x 5 = 2000 cases
 		\item Avgs over: 10 rounds (of 40 cases each) X 5 rounds of \textbf{re-trained} SB3 agents = 50 rounds 
 	\end{itemize}
 \end{itemize}
 -- selecting the model\\
 -- conducting the experiments\\
\subsection{Testing}
- Testing \texttt{env-test = wear threshold original}	\\
- Experiment file\\
- Rounds\\

\subsection{Evaluation method}
-pr, rc, f1, f1-beta

\begin{itemize}
	\item why classifction metrics
	\item why F1beta	
\end{itemize}


\subsection{Inference}
\begin{itemize}
	\item Training: SB3 is also unstable - show examples of results such as A2C/DQN 0.00 
	\item Training: SB3 is also unstable - SHOW SB3 tensorboard plots
	\item Training: SB3 is also unstable - EXCEL plots of results over the 10 rounds 	
\end{itemize}




\section{Results}\label{sec:Results}

\begin{figure}[ht]
	\begin{subfigure}{\textwidth}
	\centering
	\includegraphics[width=\linewidth]{Overall_Pr.pdf}  
	\caption{Precision}
	\label{fig:tr-ovr-pr}
	\end{subfigure} \par\smallskip

	\begin{subfigure}{\textwidth}
	\centering
	\includegraphics[width=\linewidth]{Overall_Rc.pdf}  
	\caption{Recall}
	\label{fig:tr-ovr-rc}
	\end{subfigure} \par\smallskip
	
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Overall_F1.pdf}  
		\caption{F1-score}
		\label{fig:tr-ovr-f1}
	\end{subfigure} \par\smallskip

	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Overall_F05.pdf}  
		\caption{F1-beta (0.5)}
		\label{fig:tr-ovr-f05}
	\end{subfigure}
	\caption{Overall performance -- Avg. performance over 10 rounds of model training}
	\label{fig:tr-overall}
\end{figure}

\begin{figure}[ht]
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Simulated_Pr.pdf}  
		\caption{Precision}
		\label{fig:tr-sim-pr}
	\end{subfigure} \par\smallskip
	
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Simulated_Rc.pdf}  
		\caption{Recall}
		\label{fig:tr-sim-rc}
	\end{subfigure} \par\smallskip
	
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Simulated_F1.pdf}  
		\caption{F1-score}
		\label{fig:tr-sim-f1}
	\end{subfigure} \par\smallskip
	
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Simulated_F05.pdf}  
		\caption{F1-beta (0.5)}
		\label{fig:tr-sim-f05}
	\end{subfigure}
	\caption{Simulated environment -- Avg. performance over 10 rounds of model training}
	\label{fig:tr-sim-env}
\end{figure}

\begin{figure}[ht]
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Singevariable_Pr.pdf}  
		\caption{Precision}
		\label{fig:tr-ss-pr}
	\end{subfigure} \par\smallskip
	
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Singevariable_Rc.pdf}  
		\caption{Recall}
		\label{fig:tr-ss-rc}
	\end{subfigure} \par\smallskip
	
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Singevariable_F1.pdf}  
		\caption{F1-score}
		\label{fig:tr-ss-f1}
	\end{subfigure} \par\smallskip
	
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Singevariable_F05.pdf}  
		\caption{F1-beta (0.5)}
		\label{fig:tr-ss-f05}
	\end{subfigure}
	\caption{Singe-variable state environment -- Avg. performance over 10 rounds of model training}
	\label{fig:tr-ss-env}
\end{figure}

\begin{figure}[ht]
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Multivariate_Pr.pdf}  
		\caption{Precision}
		\label{fig:tr-ms-pr}
	\end{subfigure} \par\smallskip
	
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Multivariate_Rc.pdf}  
		\caption{Recall}
		\label{fig:tr-ms-rc}
	\end{subfigure} \par\smallskip
	
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Multivariate_F1.pdf}  
		\caption{F1-score}
		\label{fig:tr-ms-f1}
	\end{subfigure} \par\smallskip
	
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Multivariate_F05.pdf}  
		\caption{F1-beta (0.5)}
		\label{fig:tr-ms-f05}
	\end{subfigure}
	\caption{Multi-variate state environment -- Avg. performance over 10 rounds of model training}
	\label{fig:tr-ms-env}
\end{figure}

\subsection{Detailed metrics}
xx
\newgeometry{margin=1cm} % Change margins for landscape table
\begin{landscape}\centering
	\begin{table*}
		\sffamily
		\rowspace{1.3}
		\begin{tabular}{@{}l rrrr c rrrr c rrrr c rrrr@{}} \arrayrulecolor{black!40}\toprule
			& \multicolumn{4}{c}{\textbf{REINFORCE}} & & \multicolumn{4}{c}{A2C} &
			& \multicolumn{4}{c}{DQN} & & \multicolumn{4}{c}{PPO} \\
			\cmidrule{2-5} \cmidrule{7-10} \cmidrule{12-15} \cmidrule{17-20}
			Environment &Prec. &Recall &F1 &F0.5 & &Prec. &Recall &F1 &F0.5 & &Prec. &Recall &F1 &F0.5 & &Prec. &Recall &F1 &F0.5\\ \midrule
			Simulated  - No noise &0.842 &0.878 &0.838 & 0.834 & & 0.424 &0.451 &0.423 &0.421 & &0.426 &0.674 &0.471 &0.410 & &0.504 &0.200 &0.271&0.360\\
			Simulated  - Low noise &0.777 &0.929 &0.834 & 0.796 & & 0.465 &0.423 &0.409 &0.427 & &0.421 &0.338 &0.270 &0.283 & &0.482 &0.236 &0.296&0.369\\
			Simulated  - High noise &0.798 &0.940 &0.851 & 0.816 & & 0.358 &0.281 &0.256 &0.272 & &0.447 &0.519 &0.380 &0.360 & &0.514 &0.207 &0.286&0.382\\ \midrule
			
			PHM C01 SS - No noise &0.478 &0.363 &0.400 & 0.439 & & 0.501 &0.500 &0.493 &0.496 & &0.472 &0.807 &0.568 &0.490 & &0.440 &0.417 &0.387&0.395\\
			PHM C01 SS - Low noise &0.507 &0.311 &0.332 & 0.383 & & 0.503 &0.598 &0.535 &0.513 & &0.393 &0.502 &0.351 &0.317 & &0.522 &0.338 &0.388&0.448\\
			PHM C01 SS - High noise &0.693 &0.562 &0.579 & 0.623 & & 0.266 &0.282 &0.267 &0.262 & &0.458 &0.525 &0.400 &0.384 & &0.456 &0.369 &0.372&0.400\\ \hdashline
			
			PHM C04 SS - No noise &0.751 &0.878 &0.784 & 0.757 & & 0.487 &0.442 &0.449 &0.463 & &0.439 &0.684 &0.472 &0.411 & &0.500 &0.510 &0.469&0.473\\
			PHM C04 SS - Low noise &0.662 &0.756 &0.672 & 0.657 & & 0.409 &0.455 &0.428 &0.416 & &0.411 &0.500 &0.370 &0.341 & &0.488 &0.280 &0.324&0.386\\
			PHM C04 SS - High noise &0.611 &0.713 &0.620 & 0.598 & & 0.518 &0.607 &0.552 &0.530 & &0.358 &0.451 &0.325 &0.294 & &0.428 &0.262 &0.286&0.333\\ \hdashline
			
			PHM C06 SS - No noise &0.830 &0.726 &0.754 & 0.792 & & 0.517 &0.509 &0.507 &0.511 & &0.360 &0.309 &0.256 &0.258 & &0.409 &0.248 &0.275&0.321\\
			PHM C06 SS - Low noise &0.205 &0.279 &0.228 & 0.212 & & 0.510 &0.577 &0.530 &0.516 & &0.434 &0.266 &0.266 &0.296 & &0.417 &0.181 &0.232&0.294\\
			PHM C06 SS - High noise &0.709 &0.843 &0.759 & 0.726 & & 0.316 &0.324 &0.311 &0.308 & &0.449 &0.518 &0.400 &0.375 & &0.388 &0.222 &0.265&0.317\\ \midrule
			
			PHM C01 MS - No noise &0.835 &0.652 &0.656 & 0.716 & & 0.461 &0.444 &0.397 &0.404 & &0.384 &0.558 &0.393 &0.348 & &0.513 &0.383 &0.416&0.460\\
			PHM C04 MS - No noise &0.739 &0.255 &0.359 & 0.494 & & 0.498 &0.589 &0.490 &0.470 & &0.323 &0.209 &0.160 &0.168 & &0.499 &0.393 &0.421&0.457\\
			PHM C06 MS - No noise &0.864 &0.356 &0.469 & 0.616 & & 0.501 &0.713 &0.578 &0.527 & &0.489 &0.705 &0.529 &0.479 & &0.523 &0.488 &0.485&0.498\\
			
			\bottomrule
		\end{tabular}
		\caption{Model performance comparison all variants of the environments, over 10 rounds of training.}
		\label{tbl:DetailedMetrics}
	\end{table*}
\end{landscape}
\restoregeometry % Restore margins after landscape table


\subsection{Super models metrics}

\newgeometry{margin=1cm} % Change margins for landscape table
\begin{landscape}\centering
	\begin{table*}
		\sffamily
		\rowspace{1.3}
		\begin{tabular}{@{}l rrrr c rrrr c rrrr c rrrr@{}} \arrayrulecolor{black!40}\toprule
			& \multicolumn{4}{c}{\textbf{REINFORCE}} & & \multicolumn{4}{c}{A2C} &
			& \multicolumn{4}{c}{DQN} & & \multicolumn{4}{c}{PPO} \\
			\cmidrule{2-5} \cmidrule{7-10} \cmidrule{12-15} \cmidrule{17-20}
			Environment &Prec. &Recall &F1 &F0.5 & &Prec. &Recall &F1 &F0.5 & &Prec. &Recall &F1 &F0.5 & &Prec. &Recall &F1 &F0.5\\ \midrule
			Simulated  - No noise &0.897 &0.960 &0.926 & 0.908 & & 0.500 &1.000 &0.667 &0.556 & &0.505 &0.980 &0.667 &0.560 & &0.669 &0.430 &0.518&0.597\\
			Simulated  - Low noise &0.960 &0.945 &0.952 & 0.957 & & 0.516 &1.000 &0.680 &0.571 & &0.500 &0.980 &0.662 &0.554 & &0.633 &0.460 &0.530&0.586\\
			Simulated  - High noise &0.922 &0.990 &0.955 & 0.935 & & 0.503 &1.000 &0.669 &0.558 & &0.504 &0.990 &0.668 &0.559 & &0.569 &0.355 &0.434&0.505\\\midrule
			
			PHM C01 SS - No noise &0.889 &0.995 &0.939 & 0.908 & & 0.586 &0.625 &0.603 &0.592 & &0.647 &0.970 &0.776 &0.693 & &0.543 &1.000 &0.703&0.597\\
			PHM C01 SS - Low noise &0.988 &0.765 &0.861 & 0.932 & & 0.499 &0.995 &0.664 &0.554 & &0.504 &0.990 &0.668 &0.559 & &0.623 &0.740 &0.675&0.643\\
			PHM C01 SS - High noise &0.850 &0.970 &0.905 & 0.871 & & 0.521 &0.680 &0.588 &0.546 & &0.505 &0.985 &0.668 &0.560 & &0.520 &0.725 &0.604&0.551\\\hdashline
			
			PHM C04 SS - No noise &0.811 &1.000 &0.895 & 0.842 & & 0.536 &0.645 &0.583 &0.554 & &0.501 &0.965 &0.660 &0.554 & &0.579 &0.895 &0.702&0.622\\
			PHM C04 SS - Low noise &0.798 &0.980 &0.879 & 0.829 & & 0.556 &0.665 &0.603 &0.573 & &0.734 &0.990 &0.843 &0.774 & &0.546 &0.660 &0.596&0.565\\
			PHM C04 SS - High noise &0.708 &0.840 &0.767 & 0.730 & & 0.521 &0.835 &0.641 &0.563 & &0.511 &0.985 &0.672 &0.565 & &0.517 &0.820 &0.633&0.558\\\hdashline
			
			PHM C06 SS - No noise &1.000 &0.895 &0.944 & 0.977 & & 0.520 &0.680 &0.587 &0.545 & &0.935 &0.975 &0.954 &0.942 & &0.587 &0.650 &0.615&0.597\\
			PHM C06 SS - Low noise &0.943 &0.795 &0.861 & 0.908 & & 0.501 &1.000 &0.668 &0.557 & &0.961 &0.725 &0.826 &0.901 & &0.552 &0.370 &0.438&0.497\\
			PHM C06 SS - High noise &0.821 &0.845 &0.831 & 0.825 & & 0.540 &0.755 &0.628 &0.572 & &0.980 &0.960 &0.969 &0.976 & &0.521 &0.615 &0.564&0.537\\\midrule
			
			PHM C01 MS - No noise &0.827 &0.995 &0.903 & 0.856 & & 0.500 &1.000 &0.667 &0.556 & &0.505 &0.985 &0.668 &0.560 & &0.512 &0.595 &0.549&0.526\\
			PHM C04 MS - No noise &0.910 &0.425 &0.577 & 0.738 & & 0.500 &1.000 &0.667 &0.556 & &0.501 &0.975 &0.662 &0.555 & &0.501 &0.635 &0.558&0.522\\
			PHM C06 MS - No noise &0.934 &0.865 &0.896 & 0.918 & & 0.500 &1.000 &0.667 &0.556 & &0.969 &0.600 &0.741 &0.863 & &0.497 &0.690 &0.577&0.526\\			
			\bottomrule
		\end{tabular}
		\caption{Super Models: Best models selected over 10 rounds of training.}
		\label{tbl:SuperModels}
	\end{table*}
\end{landscape}
\restoregeometry % Restore margins after landscape table

\subsection{Overall summary performance}

\begin{table*}[hbt!]\centering
	\sffamily
	\rowspace{1.3}
	\begin{tabular}{@{}l rr c rr c rr c rr@{}}
		\arrayrulecolor{black!40}\toprule
		& \multicolumn{2}{c}{Precision} & \phantom{i} & \multicolumn{2}{c}{Recall} & \phantom{i} & \multicolumn{2}{c}{F1-score} & \phantom{i} & \multicolumn{2}{c}{F1-beta score} \\
		\cmidrule{2-3} \cmidrule{5-6} \cmidrule{8-9} \cmidrule{11-12} 
		
		&Mean &SD & &Mean &SD & &Mean &SD& &Mean & SD\\ \midrule
		
		A2C & 0.449 & 0.088 & &0.480 & 0.084 & & 0.442 & 0.070 & &0.436 &0.071 \\
		DQN & 0.418 & 0.185 & &0.504 & 0.032 & & 0.374 & 0.035 & &0.348 &0.058 \\
		PPO & 0.472 & 0.144 & &0.316 & 0.087 & & 0.345 & 0.091 & &0.393 &0.105 \\
		REINFORCE & 0.687 & 0.059 & &0.629 & 0.051 & & 0.609 & 0.050 & &0.631 &0.052 \\
			
		\bottomrule
	\end{tabular}
	\caption{Model performance summary - averaged over all environment.}
	\label{tbl:OverallSummary}
\end{table*}

\subsection{Simulated environment}
\begin{table*}[hbt!]\centering\sffamily
	\rowspace{1.3}
	\begin{tabular}{@{}l rr c rr c rr c rr@{}}
		\arrayrulecolor{black!40}\toprule
		& \multicolumn{2}{c}{Precision} & \phantom{i} & \multicolumn{2}{c}{Recall} & \phantom{i} & \multicolumn{2}{c}{F1-score} & \phantom{i} & \multicolumn{2}{c}{F1-beta score} \\
		\cmidrule{2-3} \cmidrule{5-6} \cmidrule{8-9} \cmidrule{11-12} 
		
		&Mean &SD & &Mean &SD & &Mean &SD& &Mean & SD\\ \midrule
		A2C & 0.416 & 0.120 & &0.385 & 0.073 & & 0.363 & 0.072 & &0.373 &0.082 \\
		DQN & 0.432 & 0.184 & &0.510 & 0.031 & & 0.374 & 0.034 & &0.351 &0.056 \\
		PPO & 0.500 & 0.178 & &0.215 & 0.081 & & 0.285 & 0.099 & &0.370 &0.122 \\
		REINFORCE & 0.806 & 0.040 & &0.915 & 0.038 & & 0.841 & 0.035 & &0.816 &0.037 \\
		
		
		\bottomrule
	\end{tabular}
	\caption{Model performance summary - averaged over simulated environments.}
	\label{tbl:SimulatedEnv}
\end{table*}

\newpage
\subsection{Real data -- simple uni-variate environment}
\begin{table*}[h]\centering
	\sffamily
	\rowspace{1.3}
	\begin{tabular}{@{}l rr c rr c rr c rr@{}}
		\arrayrulecolor{black!40}\toprule
		& \multicolumn{2}{c}{Precision} & \phantom{i} & \multicolumn{2}{c}{Recall} & \phantom{i} & \multicolumn{2}{c}{F1-score} & \phantom{i} & \multicolumn{2}{c}{F1-beta score} \\
		\cmidrule{2-3} \cmidrule{5-6} \cmidrule{8-9} \cmidrule{11-12} 
		
		&Mean &SD & &Mean &SD & &Mean &SD& &Mean & SD\\ \midrule
		A2C & 0.447 & 0.077 & &0.477 & 0.091 & & 0.452 & 0.072 & &0.446 &0.070 \\
		DQN & 0.419 & 0.179 & &0.507 & 0.032 & & 0.379 & 0.036 & &0.352 &0.057 \\
		PPO & 0.450 & 0.146 & &0.314 & 0.082 & & 0.333 & 0.087 & &0.374 &0.102 \\
		REINFORCE & 0.605 & 0.046 & &0.603 & 0.046 & & 0.570 & 0.041 & &0.576 &0.040 \\
		
		\bottomrule
	\end{tabular}
	\caption{Model performance summary - averaged over PHM-2010 environments with simple single-variable environment.}
	\label{tbl:PHMSS}
\end{table*}

\subsection{Real data -- complex multi-variate state}
\begin{table*}[hbt!]\centering
	\sffamily
	\rowspace{1.3}
	\begin{tabular}{@{}l rr c rr c rr c rr@{}}
		\arrayrulecolor{black!40}\toprule
		& \multicolumn{2}{c}{Precision} & \phantom{i} & \multicolumn{2}{c}{Recall} & \phantom{i} & \multicolumn{2}{c}{F1-score} & \phantom{i} & \multicolumn{2}{c}{F1-beta score} \\
		\cmidrule{2-3} \cmidrule{5-6} \cmidrule{8-9} \cmidrule{11-12} 
		
		&Mean &SD & &Mean &SD & &Mean &SD& &Mean & SD\\ \midrule
		A2C & 0.487 & 0.086 & &0.582 & 0.075 & & 0.488 & 0.063 & &0.467 &0.065 \\
		DQN & 0.399 & 0.204 & &0.491 & 0.032 & & 0.361 & 0.035 & &0.332 &0.060 \\
		PPO & 0.512 & 0.107 & &0.422 & 0.107 & & 0.441 & 0.096 & &0.472 &0.096 \\
		REINFORCE & 0.813 & 0.119 & &0.421 & 0.079 & & 0.495 & 0.090 & &0.609 &0.101 \\
		
		
		\bottomrule
	\end{tabular}
	\caption{Model performance summary - averaged over PHM-2010 environments with complex multi-variate environment.}
	\label{tbl:PHMMS}
\end{table*}

\subsection{Hypothesis testing}

Notes: \\
- Definition of 1 data point\\
- Single training round (5th of the 10 rounds)\\
- Single dataset:  PHM C01\\
- Single env:  Simple uni-variate state\\
- Single noise setting:  HBD = high\\
- File has 40 rows = 4 algos x 10 test rounds\\
- 10 test rounds of 40 random sampled points\\
- 1 point = 1 row = 1 classification test with 40 samples\\
- E.g. Dasic = 10 training rounds x 10 test rounds x 3 noise settings = 300 sample points\\
- E.g. PHM SS = 3 data-sets x 10 training rounds x 10 test rounds x 3 noise settings = 900 sample points\\
- Technically Dasic = 300 points for metrics = 300 * 40 sampled data points of wear data = 12000\\



\begin{equation}
	\left.\begin{aligned}
		H_0 & : \mu_{RF} - \mu_{AA} = 0,\;\; \\
		H_a & : \mu_{RF} - \mu_{AA} > 0, \;\;
	\end{aligned}
	\right\}
	\qquad \forall \;\; \text{$AA \in[A2C, DQN, PPO]$}
\end{equation}

\begin{table*}[hbt!]\centering
	\sffamily
	\rowspace{1.3}
	\begin{tabular}{@{}l c rrr c l rrr @{}}
		\arrayrulecolor{black!40}\toprule
		
		&& \multicolumn{3}{c}{\textbf{p Value}} & \phantom{i} & & \multicolumn{3}{c}{\textbf{t Statistic}} \\
		\cmidrule{3-5} \cmidrule{8-10} 
		
		Metric && \small {RF $\underset{H_0}{\overset{H_a}{\geqq}}$ A2C} &\small {RF $\underset{H_0}{\overset{H_a}{\geqq}}$DQN} &\small {RF $\underset{H_0}{\overset{H_a}{\geqq}}$PPO} & & & \small {RF $\underset{H_0}{\overset{H_a}{\geqq}}$ A2C} &\small {RF $\underset{H_0}{\overset{H_a}{\geqq}}$DQN} &\small {RF $\underset{H_0}{\overset{H_a}{\geqq}}$PPO} \\ \midrule 
		\multicolumn{10}{@{}l}{\textbf{Overall} (1500 samples)} \\[6pt]
		Precision & &4.31E-126 &2.17E-109 &2.81E-106 & & &25.071 &23.170 &22.804\\
		Recall & &4.20E-35 &3.37E-16 &4.36E-150 & & &12.522 &8.206 &27.650\\
		F1 score & &1.99E-64 &1.46E-88 &5.29E-155 & & &17.364 &20.634 &28.160\\[6pt]\midrule
		
		\multicolumn{10}{@{}l}{\textbf{Simulated environment} (300 samples)}\\[6pt]
		Precision & &3.20E-98 &1.69E-63 &2.65E-81 & & &25.611 &19.032 &22.427\\
		Recall & &8.12E-104 &2.56E-41 &1.57E-264 & & &26.665 &14.558 &62.541\\
		F1 score & &9.60E-134 &8.56E-99 &2.96E-242 & & &32.402 &25.719 &56.575\\[6pt] \midrule
		
		\multicolumn{10}{@{}l}{\textbf{PHM Real data - Simple uni-variate state} (900 samples)} \\[6pt]
		Precision & &2.27E-32 &7.29E-31 &9.95E-31 & & &12.082 &11.770 &11.742\\
		Recall & &1.27E-16 &1.55E-06 &8.19E-71 & & &8.357 &4.821 &18.607\\
		F1 score & &1.94E-19 &4.67E-34 &2.19E-67 & & &9.121 &12.423 &18.098\\ [6pt]\midrule
		
		\multicolumn{10}{@{}l}{\textbf{PHM Real data - Complex multi-variate state} (300 samples)}\\[6pt]
		Precision & &1.64E-60 &3.34E-54 &7.88E-59 & & &18.451 &17.207 &18.122\\
		Recall & &2.69E-10 &2.69E-02 &9.68E-01 & & &-6.425 &-2.219 &-0.041\\
		F1 score & &7.27E-01 &1.44E-08 &1.35E-03 & & &0.349 &5.748 &3.220\\
		\bottomrule
	\end{tabular}
	\caption{One-tail t-test - Ho: No difference in metrics. Ha: REINFORCE metric > Advanced algorithm metric}
	\label{tbl:ttest}
\end{table*}



\subsection{Training times}
\begin{figure}[ht]
	\centering
	\includegraphics[width=\linewidth]{Model_training_time.pdf}  
	\caption{Training time. Across 10 rounds and all environment variants.}
	\label{fig:tr-time}
\end{figure}






\section{Discussion}\label{sec:Discussion}
\cite{duan2016benchmarking} the environment.\textcolor{red}{*OBSERVATION**}REINFORCE: Despite its simplicity, REINFORCE is an
effective algorithm in optimizing deep neural network policies
in most basic and locomotion tasks. Even for high-
DOF tasks like Ant, REINFORCE can achieve competitive
results. However we observe that REINFORCE sometimes
suffers from premature convergence to local optima
as noted by Peters  Schaal (2008), which explains the performance
gaps between REINFORCE and TNPG on tasks
such as Walker (Figure 3(a)). By visualizing the final policies,
we can see that REINFORCE results in policies that
tend to jump forward and fall over to maximize short-term
return instead of acquiring a stable walking gait to maximize
long-term return. In Figure 3(b), we can observe
that even with a small learning rate, steps taken by REINFORCE
can sometimes result in large changes to policy
distribution, which may explain the fast convergence to local
optima.


\subsection{Precision or Recall?}
- Precision => low FP => False replacement-action reduced. \textcolor{red}{***} Unnecessary tool replacement reduced. Tool life maximized, down time minimized, production disruption minimized \\
- Recall => low FN => False declaration of normal operation reduced. Reduce missed replacements. Tool replacements increased. \textcolor{red}{***} Product quality not compromised. 


limitiongs of the slecting model for evaluation

\cite{SB3-paper} -- quote from paper - intro section -- "A major challenge is that small implementation details can have a substantial effect on performance often greater than the difference between algorithms (Engstrom et al., 2020). It is particularly important that  implementations used as experimental baselines are reliable; otherwise, novel algorithms compared to weak baselines lead to inated estimates of performance improvements"

DPO paper - Rafael Rafailov May-2023 --- Direct Preference Optimization: Your Language Model is Secretly a Reward Model 

5.2 Instability of Actor-Critic Algorithms
We can also use our framework to diagnose instabilities with standard actor-critic algorithms used
for the RLHF, such as PPO. We follow the RLHF pipeline and focus on the RL fine-tuning step
outlined in Section 3. We can draw connections to the control as inference framework [18] for the
constrained RL problem outlined in 3. We assume a parameterized model and minimize
This is the same objective optimized in prior works [45, 35, 1, 23] using the DPO-equivalent reward
for the reward class of In this setting, we can interpret the normalization term in 
as the soft value function of the reference policy . While this term does not affect the optimal
solution, without it, the policy gradient of the objective could have high variance, making learning
unstable. We can accommodate for the normalization term using a learned value function, but that
can also be difficult to optimize. Alternatively, prior works have normalized rewards using a human
completion baseline, essentially a single sample Monte-Carlo estimate of the normalizing term. In
contrast the DPO reparameterization yields a reward function that does not require any baselines.

\section{Conclusion}\label{sec:Conclusion}


%% References
\bibliography{ES_bibliography}
\end{document}
